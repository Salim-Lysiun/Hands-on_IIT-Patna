{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40b0c611"
      },
      "source": [
        "## Classification of ictal and preictal processed Multi-channel EEG data(CHB-MIT) using 2-Layer of LSTM developed from Scratch"
      ],
      "id": "40b0c611"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d26582b3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import time"
      ],
      "id": "d26582b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eANh0AU-AnR2",
        "outputId": "77df3343-4e92-43ad-bbff-58d2a0f8383d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "eANh0AU-AnR2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3870bed4"
      },
      "outputs": [],
      "source": [
        "# device Configuration\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "id": "3870bed4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aa7855f"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, windowlen):\n",
        "    \"\"\"Transform a time series into a prediction dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        lookback: Size of window for prediction\n",
        "    \"\"\"\n",
        "    # feature_scalling\n",
        "    sc = StandardScaler()\n",
        "    signal = sc.fit_transform(dataset)\n",
        "    X = []\n",
        "    for i in range(int(len(signal)/windowlen)):\n",
        "        feature = signal[i*windowlen:(i+1)*windowlen]\n",
        "        X.append(feature)\n",
        "    return torch.tensor(np.array(X)).float()\n",
        "\n",
        "def data_generator( batch_size, windowlen):\n",
        "    print('Loading CHB-MIT Interical and preictal dataset...')\n",
        "    preictal_data = pd.read_csv('/content/drive/MyDrive/data/ictal_data.csv')\n",
        "    ictal_data = pd.read_csv('/content/drive/MyDrive/data/preictal_data.csv')\n",
        "\n",
        "    class1 = create_dataset(preictal_data, windowlen=windowlen)\n",
        "    y_1= torch.zeros(class1.shape[0],1)\n",
        "\n",
        "    class2 = create_dataset(ictal_data, windowlen=windowlen)\n",
        "    y_2 = torch.ones(class2.shape[0],1)\n",
        "\n",
        "    datasets = torch.cat((class1, class2),0)\n",
        "    labels = torch.cat((y_1, y_2), 0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(datasets, labels, test_size=0.25, shuffle=True, random_state=42)\n",
        "    print(f\" Shape of the Training data is {X_train.shape,}, and Testing data is {X_test.shape}\" )\n",
        "\n",
        "    train_loader = data.DataLoader(data.TensorDataset(X_train, y_train),  batch_size=batch_size, shuffle=True)\n",
        "    test_loader = data.DataLoader(data.TensorDataset(X_test, y_test),  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "id": "7aa7855f"
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, d_dim, embed_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.d_dim = d_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed = nn.Linear(self.d_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "metadata": {
        "id": "CFlhL_5JcXfI"
      },
      "id": "CFlhL_5JcXfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aabb90e",
        "outputId": "03711495-b25c-406e-ebb5-57c89904c61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CHB-MIT Interical and preictal dataset...\n",
            " Shape of the Training data is (torch.Size([24574, 64, 23]),), and Testing data is torch.Size([8192, 64, 23])\n"
          ]
        }
      ],
      "source": [
        "batch_size =50\n",
        "window_length = 64\n",
        "train_loader, test_loader = data_generator(batch_size, window_length)"
      ],
      "id": "8aabb90e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50e7c633"
      },
      "outputs": [],
      "source": [
        "X_train, label= next(iter(train_loader))\n",
        "# Hyperparameters\n",
        "num_classes =1\n",
        "num_epoch = 10\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "feature_length = X_train.shape[2]\n",
        "sequence_length = X_train.shape[1]\n",
        "embed_dim = 40\n",
        "hidden_length = 50\n",
        "log_interval =10\n",
        "lr = 0.001\n",
        "epochs = 30"
      ],
      "id": "50e7c633"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "379efc89"
      },
      "outputs": [],
      "source": [
        "class LSTM_Scratch(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple LSTM from Scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_length, hidden_length):\n",
        "        super(LSTM_Scratch, self).__init__()\n",
        "        self.feature_length = feature_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # forget gate components\n",
        "        self.linear_forget_w1 = nn.Linear(self.feature_length, self.hidden_length, bias=True)\n",
        "        self.linear_forget_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_forget = nn.Sigmoid()\n",
        "\n",
        "        # input gate components\n",
        "        self.linear_gate_w2 = nn.Linear(self.feature_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_gate = nn.Sigmoid()\n",
        "\n",
        "        # cell memory components\n",
        "        self.linear_gate_w3 = nn.Linear(self.feature_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.activation_gate = nn.Tanh()\n",
        "\n",
        "        # out gate components\n",
        "        self.linear_gate_w4 = nn.Linear(self.feature_length, self.hidden_length, bias=True)\n",
        "        self.linear_gate_r4 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_hidden_out = nn.Sigmoid()\n",
        "\n",
        "        self.activation_final = nn.Tanh()\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        x = self.linear_forget_w1(x)\n",
        "        h = self.linear_forget_r1(h)\n",
        "        return self.sigmoid_forget(x + h)\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "        # Equation 1. input gate\n",
        "        x_temp = self.linear_gate_w2(x)\n",
        "        h_temp = self.linear_gate_r2(h)\n",
        "        return self.sigmoid_gate(x_temp + h_temp)\n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        x = self.linear_gate_w3(x)\n",
        "        h = self.linear_gate_r3(h)\n",
        "\n",
        "        # new information part that will be injected in the new context\n",
        "        k = self.activation_gate(x + h)\n",
        "        g = k * i\n",
        "\n",
        "        # forget old context/cell info\n",
        "        c = f * c_prev\n",
        "        # learn new context/cell info\n",
        "        c_next = g + c\n",
        "        return c_next\n",
        "\n",
        "    def out_gate(self, x, h):\n",
        "        x = self.linear_gate_w4(x)\n",
        "        h = self.linear_gate_r4(h)\n",
        "        return self.sigmoid_hidden_out(x + h)\n",
        "\n",
        "    def forward(self, x_t, tuple_in ):\n",
        "        hidden_seq = []\n",
        "        (h, c_prev) = tuple_in\n",
        "        for t in range(sequence_length):\n",
        "            x = x_t[:,t,:]\n",
        "            # Equation 1. input gate\n",
        "            i = self.input_gate(x, h)\n",
        "\n",
        "            # Equation 2. forget gate\n",
        "            f = self.forget(x, h)\n",
        "\n",
        "            # Equation 3. updating the cell memory\n",
        "            c_next = self.cell_memory_gate(i, f, x, h,c_prev)\n",
        "\n",
        "            # Equation 4. calculate the main output gate\n",
        "            o = self.out_gate(x, h)\n",
        "\n",
        "            # Equation 5. produce next hidden output\n",
        "            h_next = o * self.activation_final(c_next)\n",
        "\n",
        "            c_prev = c_next\n",
        "            h = h_next\n",
        "\n",
        "            hidden_seq.append(h.unsqueeze(0))\n",
        "\n",
        "        #reshape hidden_seq p/ retornar\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_seq, h_next, c_next"
      ],
      "id": "379efc89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5b6bf4f"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(feature_length,embed_dim)\n",
        "        self.lstm = LSTM_Scratch(embed_dim, hidden_length) #nn.LSTM(28, 28, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_length, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        xe = self.embedding(x)\n",
        "        h_t = torch.zeros(x.size(0), hidden_length).to(device)\n",
        "        c_t = torch.zeros(x.size(0), hidden_length).to(device)\n",
        "\n",
        "\n",
        "        x_t, h_t, c_t = self.lstm(xe, (h_t, c_t))\n",
        "\n",
        "        out = self.fc1(h_t)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "id": "d5b6bf4f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae341d2c"
      },
      "outputs": [],
      "source": [
        "# Loss and Optimizer\n",
        "model= Net().to(device)\n"
      ],
      "id": "ae341d2c"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Model_{}_dim_{}_lr_{}\".format(\n",
        "            'LSTM',embed_dim, lr)\n",
        "\n",
        "message_filename =  'r_' + model_name + '.txt'\n",
        "model_filename =  'm_' + model_name + '.pt'\n",
        "with open(message_filename, 'w') as out:\n",
        "    out.write('start\\n')\n",
        "\n",
        "\n",
        "def output_s(message, save_filename):\n",
        "    print (message)\n",
        "    with open(save_filename, 'a') as out:\n",
        "        out.write(message + '\\n')\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "\n",
        "\n",
        "def train(ep):\n",
        "    targets = list()\n",
        "    preds = list()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "        pred = output.round()\n",
        "        correct += (pred== target).sum().item()\n",
        "        targets += list(target.detach().cpu().numpy())\n",
        "        preds += list(pred.detach().cpu().numpy())\n",
        "        acc = 100. * correct / ((batch_idx+1) * batch_size)\n",
        "\n",
        "        if batch_idx > 0 and batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.2f} \\t Acc: {:.2f}\".format(\n",
        "                ep, batch_idx * batch_size, len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), train_loss.item()/(batch_idx),acc))\n",
        "\n",
        "    return 100. * correct / len(train_loader.dataset), train_loss.item()/batch_size,\n",
        "\n",
        "\n",
        "## Leeanable parameters counts ###\n",
        "def test():\n",
        "    model.eval()\n",
        "\n",
        "    targets = list()\n",
        "    preds = list()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.round()\n",
        "            correct += (pred== target).sum().item()\n",
        "            targets += list(target.detach().cpu().numpy())\n",
        "            preds += list(pred.detach().cpu().numpy())\n",
        "\n",
        "        Acc = 100. * correct / len(test_loader.dataset)\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss: {:.3f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset), Acc))\n",
        "        #output_s(message, message_filename)\n",
        "        return targets, preds, Acc, test_loss\n",
        "\n",
        "#model_total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# In[112]:"
      ],
      "metadata": {
        "id": "mi47Mjmrb_-K"
      },
      "id": "mi47Mjmrb_-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    exec_time = 0\n",
        "    for epoch in range(1, epochs+1):\n",
        "        start = time.time()\n",
        "        train_acc, train_loss = train(epoch)\n",
        "        end = time.time()\n",
        "        t = end-start\n",
        "        exec_time+= t\n",
        "        preds, targets, test_acc, test_loss = test()\n",
        "        message = ('Train Epoch: {}, Train loss: {:.4f}, Time taken: {:.4f}, Train Accuracy: {:.4f}, Test loss: {:.4f}, Test Accuracy: {:.4f}' .format(\n",
        "                epoch, train_loss, t, train_acc, test_loss, test_acc))\n",
        "        output_s(message, message_filename)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            lr /= 10\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        if epoch%(epochs)==0:\n",
        "            print('Total Execution time for training:',exec_time)\n",
        "            preds = np.array(preds)\n",
        "            targets = np.array(targets)\n",
        "            conf_mat= confusion_matrix(targets, preds)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix= conf_mat)\n",
        "            disp.plot()\n",
        "            print(classification_report(targets, preds, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KcZQOWyzKDt-",
        "outputId": "daeb88d0-10f7-435b-9244-9e637676c87c"
      },
      "id": "KcZQOWyzKDt-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [500/24574 (2.03%)]\tLoss: 0.76 \t Acc: 55.64\n",
            "Train Epoch: 1 [1000/24574 (4.07%)]\tLoss: 0.73 \t Acc: 53.43\n",
            "Train Epoch: 1 [1500/24574 (6.10%)]\tLoss: 0.72 \t Acc: 52.97\n",
            "Train Epoch: 1 [2000/24574 (8.13%)]\tLoss: 0.71 \t Acc: 53.95\n",
            "Train Epoch: 1 [2500/24574 (10.16%)]\tLoss: 0.71 \t Acc: 53.65\n",
            "Train Epoch: 1 [3000/24574 (12.20%)]\tLoss: 0.70 \t Acc: 54.16\n",
            "Train Epoch: 1 [3500/24574 (14.23%)]\tLoss: 0.70 \t Acc: 54.17\n",
            "Train Epoch: 1 [4000/24574 (16.26%)]\tLoss: 0.70 \t Acc: 54.67\n",
            "Train Epoch: 1 [4500/24574 (18.29%)]\tLoss: 0.70 \t Acc: 55.19\n",
            "Train Epoch: 1 [5000/24574 (20.33%)]\tLoss: 0.70 \t Acc: 55.43\n",
            "Train Epoch: 1 [5500/24574 (22.36%)]\tLoss: 0.69 \t Acc: 55.26\n",
            "Train Epoch: 1 [6000/24574 (24.39%)]\tLoss: 0.69 \t Acc: 55.40\n",
            "Train Epoch: 1 [6500/24574 (26.42%)]\tLoss: 0.69 \t Acc: 55.42\n",
            "Train Epoch: 1 [7000/24574 (28.46%)]\tLoss: 0.69 \t Acc: 55.42\n",
            "Train Epoch: 1 [7500/24574 (30.49%)]\tLoss: 0.69 \t Acc: 55.72\n",
            "Train Epoch: 1 [8000/24574 (32.52%)]\tLoss: 0.69 \t Acc: 56.22\n",
            "Train Epoch: 1 [8500/24574 (34.55%)]\tLoss: 0.69 \t Acc: 56.62\n",
            "Train Epoch: 1 [9000/24574 (36.59%)]\tLoss: 0.69 \t Acc: 56.66\n",
            "Train Epoch: 1 [9500/24574 (38.62%)]\tLoss: 0.68 \t Acc: 57.19\n",
            "Train Epoch: 1 [10000/24574 (40.65%)]\tLoss: 0.68 \t Acc: 57.48\n",
            "Train Epoch: 1 [10500/24574 (42.68%)]\tLoss: 0.68 \t Acc: 57.77\n",
            "Train Epoch: 1 [11000/24574 (44.72%)]\tLoss: 0.68 \t Acc: 58.04\n",
            "Train Epoch: 1 [11500/24574 (46.75%)]\tLoss: 0.68 \t Acc: 58.48\n",
            "Train Epoch: 1 [12000/24574 (48.78%)]\tLoss: 0.67 \t Acc: 58.94\n",
            "Train Epoch: 1 [12500/24574 (50.81%)]\tLoss: 0.67 \t Acc: 59.21\n",
            "Train Epoch: 1 [13000/24574 (52.85%)]\tLoss: 0.67 \t Acc: 59.53\n",
            "Train Epoch: 1 [13500/24574 (54.88%)]\tLoss: 0.67 \t Acc: 59.62\n",
            "Train Epoch: 1 [14000/24574 (56.91%)]\tLoss: 0.67 \t Acc: 59.86\n",
            "Train Epoch: 1 [14500/24574 (58.94%)]\tLoss: 0.67 \t Acc: 59.81\n",
            "Train Epoch: 1 [15000/24574 (60.98%)]\tLoss: 0.67 \t Acc: 59.78\n",
            "Train Epoch: 1 [15500/24574 (63.01%)]\tLoss: 0.67 \t Acc: 59.83\n",
            "Train Epoch: 1 [16000/24574 (65.04%)]\tLoss: 0.67 \t Acc: 59.95\n",
            "Train Epoch: 1 [16500/24574 (67.07%)]\tLoss: 0.67 \t Acc: 60.05\n",
            "Train Epoch: 1 [17000/24574 (69.11%)]\tLoss: 0.67 \t Acc: 60.36\n",
            "Train Epoch: 1 [17500/24574 (71.14%)]\tLoss: 0.67 \t Acc: 60.68\n",
            "Train Epoch: 1 [18000/24574 (73.17%)]\tLoss: 0.66 \t Acc: 60.83\n",
            "Train Epoch: 1 [18500/24574 (75.20%)]\tLoss: 0.66 \t Acc: 61.02\n",
            "Train Epoch: 1 [19000/24574 (77.24%)]\tLoss: 0.66 \t Acc: 61.26\n",
            "Train Epoch: 1 [19500/24574 (79.27%)]\tLoss: 0.66 \t Acc: 61.41\n",
            "Train Epoch: 1 [20000/24574 (81.30%)]\tLoss: 0.66 \t Acc: 61.57\n",
            "Train Epoch: 1 [20500/24574 (83.33%)]\tLoss: 0.66 \t Acc: 61.70\n",
            "Train Epoch: 1 [21000/24574 (85.37%)]\tLoss: 0.66 \t Acc: 61.93\n",
            "Train Epoch: 1 [21500/24574 (87.40%)]\tLoss: 0.66 \t Acc: 62.11\n",
            "Train Epoch: 1 [22000/24574 (89.43%)]\tLoss: 0.65 \t Acc: 62.24\n",
            "Train Epoch: 1 [22500/24574 (91.46%)]\tLoss: 0.65 \t Acc: 62.43\n",
            "Train Epoch: 1 [23000/24574 (93.50%)]\tLoss: 0.65 \t Acc: 62.59\n",
            "Train Epoch: 1 [23500/24574 (95.53%)]\tLoss: 0.65 \t Acc: 62.76\n",
            "Train Epoch: 1 [24000/24574 (97.56%)]\tLoss: 0.65 \t Acc: 62.90\n",
            "Train Epoch: 1 [24500/24574 (99.59%)]\tLoss: 0.65 \t Acc: 63.04\n",
            "\n",
            "Test set: Average loss: 0.012, Accuracy: 5672/8192 (69.24%)\n",
            "\n",
            "Train Epoch: 1, Train loss: 6.3704, Time taken: 47.1186, Train Accuracy: 63.0422, Test loss: 0.0119, Test Accuracy: 69.2383\n",
            "Train Epoch: 2 [500/24574 (2.03%)]\tLoss: 0.64 \t Acc: 70.36\n",
            "Train Epoch: 2 [1000/24574 (4.07%)]\tLoss: 0.61 \t Acc: 70.95\n",
            "Train Epoch: 2 [1500/24574 (6.10%)]\tLoss: 0.60 \t Acc: 71.03\n",
            "Train Epoch: 2 [2000/24574 (8.13%)]\tLoss: 0.59 \t Acc: 71.41\n",
            "Train Epoch: 2 [2500/24574 (10.16%)]\tLoss: 0.60 \t Acc: 70.67\n",
            "Train Epoch: 2 [3000/24574 (12.20%)]\tLoss: 0.61 \t Acc: 70.00\n",
            "Train Epoch: 2 [3500/24574 (14.23%)]\tLoss: 0.61 \t Acc: 70.31\n",
            "Train Epoch: 2 [4000/24574 (16.26%)]\tLoss: 0.61 \t Acc: 70.22\n",
            "Train Epoch: 2 [4500/24574 (18.29%)]\tLoss: 0.61 \t Acc: 69.74\n",
            "Train Epoch: 2 [5000/24574 (20.33%)]\tLoss: 0.61 \t Acc: 69.45\n",
            "Train Epoch: 2 [5500/24574 (22.36%)]\tLoss: 0.62 \t Acc: 69.14\n",
            "Train Epoch: 2 [6000/24574 (24.39%)]\tLoss: 0.62 \t Acc: 68.88\n",
            "Train Epoch: 2 [6500/24574 (26.42%)]\tLoss: 0.62 \t Acc: 68.95\n",
            "Train Epoch: 2 [7000/24574 (28.46%)]\tLoss: 0.62 \t Acc: 68.85\n",
            "Train Epoch: 2 [7500/24574 (30.49%)]\tLoss: 0.62 \t Acc: 68.77\n",
            "Train Epoch: 2 [8000/24574 (32.52%)]\tLoss: 0.62 \t Acc: 68.75\n",
            "Train Epoch: 2 [8500/24574 (34.55%)]\tLoss: 0.62 \t Acc: 68.89\n",
            "Train Epoch: 2 [9000/24574 (36.59%)]\tLoss: 0.62 \t Acc: 68.82\n",
            "Train Epoch: 2 [9500/24574 (38.62%)]\tLoss: 0.62 \t Acc: 68.82\n",
            "Train Epoch: 2 [10000/24574 (40.65%)]\tLoss: 0.62 \t Acc: 68.72\n",
            "Train Epoch: 2 [10500/24574 (42.68%)]\tLoss: 0.61 \t Acc: 68.73\n",
            "Train Epoch: 2 [11000/24574 (44.72%)]\tLoss: 0.61 \t Acc: 68.89\n",
            "Train Epoch: 2 [11500/24574 (46.75%)]\tLoss: 0.61 \t Acc: 69.01\n",
            "Train Epoch: 2 [12000/24574 (48.78%)]\tLoss: 0.61 \t Acc: 69.04\n",
            "Train Epoch: 2 [12500/24574 (50.81%)]\tLoss: 0.61 \t Acc: 69.03\n",
            "Train Epoch: 2 [13000/24574 (52.85%)]\tLoss: 0.61 \t Acc: 69.08\n",
            "Train Epoch: 2 [13500/24574 (54.88%)]\tLoss: 0.61 \t Acc: 69.13\n",
            "Train Epoch: 2 [14000/24574 (56.91%)]\tLoss: 0.61 \t Acc: 69.25\n",
            "Train Epoch: 2 [14500/24574 (58.94%)]\tLoss: 0.61 \t Acc: 69.33\n",
            "Train Epoch: 2 [15000/24574 (60.98%)]\tLoss: 0.60 \t Acc: 69.51\n",
            "Train Epoch: 2 [15500/24574 (63.01%)]\tLoss: 0.60 \t Acc: 69.61\n",
            "Train Epoch: 2 [16000/24574 (65.04%)]\tLoss: 0.60 \t Acc: 69.78\n",
            "Train Epoch: 2 [16500/24574 (67.07%)]\tLoss: 0.60 \t Acc: 69.90\n",
            "Train Epoch: 2 [17000/24574 (69.11%)]\tLoss: 0.60 \t Acc: 69.92\n",
            "Train Epoch: 2 [17500/24574 (71.14%)]\tLoss: 0.60 \t Acc: 70.09\n",
            "Train Epoch: 2 [18000/24574 (73.17%)]\tLoss: 0.60 \t Acc: 70.12\n",
            "Train Epoch: 2 [18500/24574 (75.20%)]\tLoss: 0.59 \t Acc: 70.25\n",
            "Train Epoch: 2 [19000/24574 (77.24%)]\tLoss: 0.59 \t Acc: 70.36\n",
            "Train Epoch: 2 [19500/24574 (79.27%)]\tLoss: 0.59 \t Acc: 70.43\n",
            "Train Epoch: 2 [20000/24574 (81.30%)]\tLoss: 0.59 \t Acc: 70.45\n",
            "Train Epoch: 2 [20500/24574 (83.33%)]\tLoss: 0.59 \t Acc: 70.63\n",
            "Train Epoch: 2 [21000/24574 (85.37%)]\tLoss: 0.59 \t Acc: 70.71\n",
            "Train Epoch: 2 [21500/24574 (87.40%)]\tLoss: 0.59 \t Acc: 70.77\n",
            "Train Epoch: 2 [22000/24574 (89.43%)]\tLoss: 0.59 \t Acc: 70.66\n",
            "Train Epoch: 2 [22500/24574 (91.46%)]\tLoss: 0.60 \t Acc: 70.31\n",
            "Train Epoch: 2 [23000/24574 (93.50%)]\tLoss: 0.60 \t Acc: 69.91\n",
            "Train Epoch: 2 [23500/24574 (95.53%)]\tLoss: 0.60 \t Acc: 69.61\n",
            "Train Epoch: 2 [24000/24574 (97.56%)]\tLoss: 0.60 \t Acc: 69.50\n",
            "Train Epoch: 2 [24500/24574 (99.59%)]\tLoss: 0.60 \t Acc: 69.36\n",
            "\n",
            "Test set: Average loss: 0.013, Accuracy: 5216/8192 (63.67%)\n",
            "\n",
            "Train Epoch: 2, Train loss: 5.9117, Time taken: 46.6705, Train Accuracy: 69.3416, Test loss: 0.0131, Test Accuracy: 63.6719\n",
            "Train Epoch: 3 [500/24574 (2.03%)]\tLoss: 0.71 \t Acc: 64.00\n",
            "Train Epoch: 3 [1000/24574 (4.07%)]\tLoss: 0.68 \t Acc: 64.67\n",
            "Train Epoch: 3 [1500/24574 (6.10%)]\tLoss: 0.67 \t Acc: 63.42\n",
            "Train Epoch: 3 [2000/24574 (8.13%)]\tLoss: 0.67 \t Acc: 64.24\n",
            "Train Epoch: 3 [2500/24574 (10.16%)]\tLoss: 0.66 \t Acc: 65.69\n",
            "Train Epoch: 3 [3000/24574 (12.20%)]\tLoss: 0.65 \t Acc: 65.67\n",
            "Train Epoch: 3 [3500/24574 (14.23%)]\tLoss: 0.65 \t Acc: 64.96\n",
            "Train Epoch: 3 [4000/24574 (16.26%)]\tLoss: 0.65 \t Acc: 65.06\n",
            "Train Epoch: 3 [4500/24574 (18.29%)]\tLoss: 0.65 \t Acc: 65.34\n",
            "Train Epoch: 3 [5000/24574 (20.33%)]\tLoss: 0.64 \t Acc: 65.39\n",
            "Train Epoch: 3 [5500/24574 (22.36%)]\tLoss: 0.64 \t Acc: 65.71\n",
            "Train Epoch: 3 [6000/24574 (24.39%)]\tLoss: 0.64 \t Acc: 66.03\n",
            "Train Epoch: 3 [6500/24574 (26.42%)]\tLoss: 0.64 \t Acc: 66.47\n",
            "Train Epoch: 3 [7000/24574 (28.46%)]\tLoss: 0.64 \t Acc: 66.38\n",
            "Train Epoch: 3 [7500/24574 (30.49%)]\tLoss: 0.64 \t Acc: 66.30\n",
            "Train Epoch: 3 [8000/24574 (32.52%)]\tLoss: 0.64 \t Acc: 65.93\n",
            "Train Epoch: 3 [8500/24574 (34.55%)]\tLoss: 0.63 \t Acc: 66.01\n",
            "Train Epoch: 3 [9000/24574 (36.59%)]\tLoss: 0.64 \t Acc: 65.96\n",
            "Train Epoch: 3 [9500/24574 (38.62%)]\tLoss: 0.63 \t Acc: 66.09\n",
            "Train Epoch: 3 [10000/24574 (40.65%)]\tLoss: 0.63 \t Acc: 66.18\n",
            "Train Epoch: 3 [10500/24574 (42.68%)]\tLoss: 0.63 \t Acc: 66.18\n",
            "Train Epoch: 3 [11000/24574 (44.72%)]\tLoss: 0.63 \t Acc: 66.21\n",
            "Train Epoch: 3 [11500/24574 (46.75%)]\tLoss: 0.63 \t Acc: 66.25\n",
            "Train Epoch: 3 [12000/24574 (48.78%)]\tLoss: 0.63 \t Acc: 66.28\n",
            "Train Epoch: 3 [12500/24574 (50.81%)]\tLoss: 0.63 \t Acc: 66.40\n",
            "Train Epoch: 3 [13000/24574 (52.85%)]\tLoss: 0.63 \t Acc: 66.47\n",
            "Train Epoch: 3 [13500/24574 (54.88%)]\tLoss: 0.63 \t Acc: 66.61\n",
            "Train Epoch: 3 [14000/24574 (56.91%)]\tLoss: 0.63 \t Acc: 66.63\n",
            "Train Epoch: 3 [14500/24574 (58.94%)]\tLoss: 0.63 \t Acc: 66.58\n",
            "Train Epoch: 3 [15000/24574 (60.98%)]\tLoss: 0.63 \t Acc: 66.64\n",
            "Train Epoch: 3 [15500/24574 (63.01%)]\tLoss: 0.62 \t Acc: 66.75\n",
            "Train Epoch: 3 [16000/24574 (65.04%)]\tLoss: 0.62 \t Acc: 66.72\n",
            "Train Epoch: 3 [16500/24574 (67.07%)]\tLoss: 0.62 \t Acc: 66.78\n",
            "Train Epoch: 3 [17000/24574 (69.11%)]\tLoss: 0.62 \t Acc: 66.86\n",
            "Train Epoch: 3 [17500/24574 (71.14%)]\tLoss: 0.62 \t Acc: 66.88\n",
            "Train Epoch: 3 [18000/24574 (73.17%)]\tLoss: 0.62 \t Acc: 66.85\n",
            "Train Epoch: 3 [18500/24574 (75.20%)]\tLoss: 0.62 \t Acc: 66.96\n",
            "Train Epoch: 3 [19000/24574 (77.24%)]\tLoss: 0.62 \t Acc: 67.14\n",
            "Train Epoch: 3 [19500/24574 (79.27%)]\tLoss: 0.62 \t Acc: 67.24\n",
            "Train Epoch: 3 [20000/24574 (81.30%)]\tLoss: 0.62 \t Acc: 67.35\n",
            "Train Epoch: 3 [20500/24574 (83.33%)]\tLoss: 0.62 \t Acc: 67.49\n",
            "Train Epoch: 3 [21000/24574 (85.37%)]\tLoss: 0.61 \t Acc: 67.60\n",
            "Train Epoch: 3 [21500/24574 (87.40%)]\tLoss: 0.61 \t Acc: 67.64\n",
            "Train Epoch: 3 [22000/24574 (89.43%)]\tLoss: 0.61 \t Acc: 67.75\n",
            "Train Epoch: 3 [22500/24574 (91.46%)]\tLoss: 0.61 \t Acc: 67.83\n",
            "Train Epoch: 3 [23000/24574 (93.50%)]\tLoss: 0.61 \t Acc: 67.80\n",
            "Train Epoch: 3 [23500/24574 (95.53%)]\tLoss: 0.61 \t Acc: 67.89\n",
            "Train Epoch: 3 [24000/24574 (97.56%)]\tLoss: 0.61 \t Acc: 67.95\n",
            "Train Epoch: 3 [24500/24574 (99.59%)]\tLoss: 0.61 \t Acc: 68.02\n",
            "\n",
            "Test set: Average loss: 0.011, Accuracy: 5949/8192 (72.62%)\n",
            "\n",
            "Train Epoch: 3, Train loss: 5.9827, Time taken: 44.7475, Train Accuracy: 68.0190, Test loss: 0.0112, Test Accuracy: 72.6196\n",
            "Train Epoch: 4 [500/24574 (2.03%)]\tLoss: 0.62 \t Acc: 70.55\n",
            "Train Epoch: 4 [1000/24574 (4.07%)]\tLoss: 0.58 \t Acc: 72.10\n",
            "Train Epoch: 4 [1500/24574 (6.10%)]\tLoss: 0.56 \t Acc: 73.61\n",
            "Train Epoch: 4 [2000/24574 (8.13%)]\tLoss: 0.56 \t Acc: 74.05\n",
            "Train Epoch: 4 [2500/24574 (10.16%)]\tLoss: 0.56 \t Acc: 73.96\n",
            "Train Epoch: 4 [3000/24574 (12.20%)]\tLoss: 0.55 \t Acc: 74.07\n",
            "Train Epoch: 4 [3500/24574 (14.23%)]\tLoss: 0.55 \t Acc: 74.20\n",
            "Train Epoch: 4 [4000/24574 (16.26%)]\tLoss: 0.55 \t Acc: 74.35\n",
            "Train Epoch: 4 [4500/24574 (18.29%)]\tLoss: 0.55 \t Acc: 74.22\n",
            "Train Epoch: 4 [5000/24574 (20.33%)]\tLoss: 0.55 \t Acc: 74.20\n",
            "Train Epoch: 4 [5500/24574 (22.36%)]\tLoss: 0.55 \t Acc: 74.20\n",
            "Train Epoch: 4 [6000/24574 (24.39%)]\tLoss: 0.55 \t Acc: 73.98\n",
            "Train Epoch: 4 [6500/24574 (26.42%)]\tLoss: 0.55 \t Acc: 73.85\n",
            "Train Epoch: 4 [7000/24574 (28.46%)]\tLoss: 0.55 \t Acc: 74.00\n",
            "Train Epoch: 4 [7500/24574 (30.49%)]\tLoss: 0.55 \t Acc: 74.07\n",
            "Train Epoch: 4 [8000/24574 (32.52%)]\tLoss: 0.55 \t Acc: 74.07\n",
            "Train Epoch: 4 [8500/24574 (34.55%)]\tLoss: 0.55 \t Acc: 74.16\n",
            "Train Epoch: 4 [9000/24574 (36.59%)]\tLoss: 0.55 \t Acc: 74.31\n",
            "Train Epoch: 4 [9500/24574 (38.62%)]\tLoss: 0.55 \t Acc: 74.24\n",
            "Train Epoch: 4 [10000/24574 (40.65%)]\tLoss: 0.54 \t Acc: 74.45\n",
            "Train Epoch: 4 [10500/24574 (42.68%)]\tLoss: 0.54 \t Acc: 74.38\n",
            "Train Epoch: 4 [11000/24574 (44.72%)]\tLoss: 0.54 \t Acc: 74.46\n",
            "Train Epoch: 4 [11500/24574 (46.75%)]\tLoss: 0.54 \t Acc: 74.60\n",
            "Train Epoch: 4 [12000/24574 (48.78%)]\tLoss: 0.54 \t Acc: 74.70\n",
            "Train Epoch: 4 [12500/24574 (50.81%)]\tLoss: 0.54 \t Acc: 74.74\n",
            "Train Epoch: 4 [13000/24574 (52.85%)]\tLoss: 0.54 \t Acc: 74.74\n",
            "Train Epoch: 4 [13500/24574 (54.88%)]\tLoss: 0.54 \t Acc: 74.70\n",
            "Train Epoch: 4 [14000/24574 (56.91%)]\tLoss: 0.54 \t Acc: 74.74\n",
            "Train Epoch: 4 [14500/24574 (58.94%)]\tLoss: 0.54 \t Acc: 74.78\n",
            "Train Epoch: 4 [15000/24574 (60.98%)]\tLoss: 0.54 \t Acc: 74.70\n",
            "Train Epoch: 4 [15500/24574 (63.01%)]\tLoss: 0.54 \t Acc: 74.67\n",
            "Train Epoch: 4 [16000/24574 (65.04%)]\tLoss: 0.54 \t Acc: 74.74\n",
            "Train Epoch: 4 [16500/24574 (67.07%)]\tLoss: 0.53 \t Acc: 74.83\n",
            "Train Epoch: 4 [17000/24574 (69.11%)]\tLoss: 0.54 \t Acc: 74.81\n",
            "Train Epoch: 4 [17500/24574 (71.14%)]\tLoss: 0.53 \t Acc: 74.87\n",
            "Train Epoch: 4 [18000/24574 (73.17%)]\tLoss: 0.53 \t Acc: 74.97\n",
            "Train Epoch: 4 [18500/24574 (75.20%)]\tLoss: 0.53 \t Acc: 74.94\n",
            "Train Epoch: 4 [19000/24574 (77.24%)]\tLoss: 0.53 \t Acc: 74.92\n",
            "Train Epoch: 4 [19500/24574 (79.27%)]\tLoss: 0.53 \t Acc: 74.97\n",
            "Train Epoch: 4 [20000/24574 (81.30%)]\tLoss: 0.53 \t Acc: 75.01\n",
            "Train Epoch: 4 [20500/24574 (83.33%)]\tLoss: 0.53 \t Acc: 74.98\n",
            "Train Epoch: 4 [21000/24574 (85.37%)]\tLoss: 0.53 \t Acc: 75.04\n",
            "Train Epoch: 4 [21500/24574 (87.40%)]\tLoss: 0.53 \t Acc: 75.06\n",
            "Train Epoch: 4 [22000/24574 (89.43%)]\tLoss: 0.53 \t Acc: 75.11\n",
            "Train Epoch: 4 [22500/24574 (91.46%)]\tLoss: 0.53 \t Acc: 75.14\n",
            "Train Epoch: 4 [23000/24574 (93.50%)]\tLoss: 0.53 \t Acc: 75.09\n",
            "Train Epoch: 4 [23500/24574 (95.53%)]\tLoss: 0.53 \t Acc: 75.18\n",
            "Train Epoch: 4 [24000/24574 (97.56%)]\tLoss: 0.53 \t Acc: 75.23\n",
            "Train Epoch: 4 [24500/24574 (99.59%)]\tLoss: 0.53 \t Acc: 75.28\n",
            "\n",
            "Test set: Average loss: 0.010, Accuracy: 6415/8192 (78.31%)\n",
            "\n",
            "Train Epoch: 4, Train loss: 5.1726, Time taken: 44.9652, Train Accuracy: 75.2747, Test loss: 0.0097, Test Accuracy: 78.3081\n",
            "Train Epoch: 5 [500/24574 (2.03%)]\tLoss: 0.55 \t Acc: 78.55\n",
            "Train Epoch: 5 [1000/24574 (4.07%)]\tLoss: 0.50 \t Acc: 79.33\n",
            "Train Epoch: 5 [1500/24574 (6.10%)]\tLoss: 0.49 \t Acc: 79.87\n",
            "Train Epoch: 5 [2000/24574 (8.13%)]\tLoss: 0.49 \t Acc: 79.12\n",
            "Train Epoch: 5 [2500/24574 (10.16%)]\tLoss: 0.49 \t Acc: 78.55\n",
            "Train Epoch: 5 [3000/24574 (12.20%)]\tLoss: 0.49 \t Acc: 78.39\n",
            "Train Epoch: 5 [3500/24574 (14.23%)]\tLoss: 0.49 \t Acc: 78.65\n",
            "Train Epoch: 5 [4000/24574 (16.26%)]\tLoss: 0.49 \t Acc: 78.35\n",
            "Train Epoch: 5 [4500/24574 (18.29%)]\tLoss: 0.49 \t Acc: 78.40\n",
            "Train Epoch: 5 [5000/24574 (20.33%)]\tLoss: 0.49 \t Acc: 78.18\n",
            "Train Epoch: 5 [5500/24574 (22.36%)]\tLoss: 0.49 \t Acc: 77.80\n",
            "Train Epoch: 5 [6000/24574 (24.39%)]\tLoss: 0.49 \t Acc: 78.02\n",
            "Train Epoch: 5 [6500/24574 (26.42%)]\tLoss: 0.49 \t Acc: 78.11\n",
            "Train Epoch: 5 [7000/24574 (28.46%)]\tLoss: 0.49 \t Acc: 78.30\n",
            "Train Epoch: 5 [7500/24574 (30.49%)]\tLoss: 0.48 \t Acc: 78.58\n",
            "Train Epoch: 5 [8000/24574 (32.52%)]\tLoss: 0.48 \t Acc: 78.56\n",
            "Train Epoch: 5 [8500/24574 (34.55%)]\tLoss: 0.48 \t Acc: 78.64\n",
            "Train Epoch: 5 [9000/24574 (36.59%)]\tLoss: 0.48 \t Acc: 78.57\n",
            "Train Epoch: 5 [9500/24574 (38.62%)]\tLoss: 0.48 \t Acc: 78.55\n",
            "Train Epoch: 5 [10000/24574 (40.65%)]\tLoss: 0.48 \t Acc: 78.57\n",
            "Train Epoch: 5 [10500/24574 (42.68%)]\tLoss: 0.48 \t Acc: 78.63\n",
            "Train Epoch: 5 [11000/24574 (44.72%)]\tLoss: 0.48 \t Acc: 78.54\n",
            "Train Epoch: 5 [11500/24574 (46.75%)]\tLoss: 0.48 \t Acc: 78.52\n",
            "Train Epoch: 5 [12000/24574 (48.78%)]\tLoss: 0.48 \t Acc: 78.61\n",
            "Train Epoch: 5 [12500/24574 (50.81%)]\tLoss: 0.48 \t Acc: 78.62\n",
            "Train Epoch: 5 [13000/24574 (52.85%)]\tLoss: 0.48 \t Acc: 78.49\n",
            "Train Epoch: 5 [13500/24574 (54.88%)]\tLoss: 0.48 \t Acc: 78.54\n",
            "Train Epoch: 5 [14000/24574 (56.91%)]\tLoss: 0.48 \t Acc: 78.45\n",
            "Train Epoch: 5 [14500/24574 (58.94%)]\tLoss: 0.48 \t Acc: 78.48\n",
            "Train Epoch: 5 [15000/24574 (60.98%)]\tLoss: 0.48 \t Acc: 78.54\n",
            "Train Epoch: 5 [15500/24574 (63.01%)]\tLoss: 0.48 \t Acc: 78.64\n",
            "Train Epoch: 5 [16000/24574 (65.04%)]\tLoss: 0.48 \t Acc: 78.62\n",
            "Train Epoch: 5 [16500/24574 (67.07%)]\tLoss: 0.48 \t Acc: 78.60\n",
            "Train Epoch: 5 [17000/24574 (69.11%)]\tLoss: 0.47 \t Acc: 78.76\n",
            "Train Epoch: 5 [17500/24574 (71.14%)]\tLoss: 0.47 \t Acc: 78.83\n",
            "Train Epoch: 5 [18000/24574 (73.17%)]\tLoss: 0.47 \t Acc: 78.93\n",
            "Train Epoch: 5 [18500/24574 (75.20%)]\tLoss: 0.47 \t Acc: 78.97\n",
            "Train Epoch: 5 [19000/24574 (77.24%)]\tLoss: 0.47 \t Acc: 78.98\n",
            "Train Epoch: 5 [19500/24574 (79.27%)]\tLoss: 0.47 \t Acc: 78.94\n",
            "Train Epoch: 5 [20000/24574 (81.30%)]\tLoss: 0.47 \t Acc: 79.00\n",
            "Train Epoch: 5 [20500/24574 (83.33%)]\tLoss: 0.47 \t Acc: 79.06\n",
            "Train Epoch: 5 [21000/24574 (85.37%)]\tLoss: 0.47 \t Acc: 79.15\n",
            "Train Epoch: 5 [21500/24574 (87.40%)]\tLoss: 0.47 \t Acc: 79.15\n",
            "Train Epoch: 5 [22000/24574 (89.43%)]\tLoss: 0.46 \t Acc: 79.29\n",
            "Train Epoch: 5 [22500/24574 (91.46%)]\tLoss: 0.46 \t Acc: 79.36\n",
            "Train Epoch: 5 [23000/24574 (93.50%)]\tLoss: 0.46 \t Acc: 79.43\n",
            "Train Epoch: 5 [23500/24574 (95.53%)]\tLoss: 0.46 \t Acc: 79.50\n",
            "Train Epoch: 5 [24000/24574 (97.56%)]\tLoss: 0.46 \t Acc: 79.48\n",
            "Train Epoch: 5 [24500/24574 (99.59%)]\tLoss: 0.46 \t Acc: 79.48\n",
            "\n",
            "Test set: Average loss: 0.009, Accuracy: 6543/8192 (79.87%)\n",
            "\n",
            "Train Epoch: 5, Train loss: 4.5434, Time taken: 47.3935, Train Accuracy: 79.4864, Test loss: 0.0091, Test Accuracy: 79.8706\n",
            "Train Epoch: 6 [500/24574 (2.03%)]\tLoss: 0.46 \t Acc: 82.18\n",
            "Train Epoch: 6 [1000/24574 (4.07%)]\tLoss: 0.44 \t Acc: 80.95\n",
            "Train Epoch: 6 [1500/24574 (6.10%)]\tLoss: 0.44 \t Acc: 80.84\n",
            "Train Epoch: 6 [2000/24574 (8.13%)]\tLoss: 0.44 \t Acc: 80.73\n",
            "Train Epoch: 6 [2500/24574 (10.16%)]\tLoss: 0.43 \t Acc: 81.18\n",
            "Train Epoch: 6 [3000/24574 (12.20%)]\tLoss: 0.43 \t Acc: 81.15\n",
            "Train Epoch: 6 [3500/24574 (14.23%)]\tLoss: 0.43 \t Acc: 81.66\n",
            "Train Epoch: 6 [4000/24574 (16.26%)]\tLoss: 0.43 \t Acc: 81.53\n",
            "Train Epoch: 6 [4500/24574 (18.29%)]\tLoss: 0.43 \t Acc: 81.41\n",
            "Train Epoch: 6 [5000/24574 (20.33%)]\tLoss: 0.43 \t Acc: 81.49\n",
            "Train Epoch: 6 [5500/24574 (22.36%)]\tLoss: 0.43 \t Acc: 81.46\n",
            "Train Epoch: 6 [6000/24574 (24.39%)]\tLoss: 0.43 \t Acc: 81.65\n",
            "Train Epoch: 6 [6500/24574 (26.42%)]\tLoss: 0.43 \t Acc: 81.44\n",
            "Train Epoch: 6 [7000/24574 (28.46%)]\tLoss: 0.43 \t Acc: 81.69\n",
            "Train Epoch: 6 [7500/24574 (30.49%)]\tLoss: 0.42 \t Acc: 81.87\n",
            "Train Epoch: 6 [8000/24574 (32.52%)]\tLoss: 0.42 \t Acc: 82.14\n",
            "Train Epoch: 6 [8500/24574 (34.55%)]\tLoss: 0.42 \t Acc: 82.26\n",
            "Train Epoch: 6 [9000/24574 (36.59%)]\tLoss: 0.42 \t Acc: 82.36\n",
            "Train Epoch: 6 [9500/24574 (38.62%)]\tLoss: 0.41 \t Acc: 82.49\n",
            "Train Epoch: 6 [10000/24574 (40.65%)]\tLoss: 0.41 \t Acc: 82.55\n",
            "Train Epoch: 6 [10500/24574 (42.68%)]\tLoss: 0.41 \t Acc: 82.64\n",
            "Train Epoch: 6 [11000/24574 (44.72%)]\tLoss: 0.41 \t Acc: 82.63\n",
            "Train Epoch: 6 [11500/24574 (46.75%)]\tLoss: 0.41 \t Acc: 82.57\n",
            "Train Epoch: 6 [12000/24574 (48.78%)]\tLoss: 0.41 \t Acc: 82.65\n",
            "Train Epoch: 6 [12500/24574 (50.81%)]\tLoss: 0.41 \t Acc: 82.83\n",
            "Train Epoch: 6 [13000/24574 (52.85%)]\tLoss: 0.41 \t Acc: 82.78\n",
            "Train Epoch: 6 [13500/24574 (54.88%)]\tLoss: 0.40 \t Acc: 82.87\n",
            "Train Epoch: 6 [14000/24574 (56.91%)]\tLoss: 0.40 \t Acc: 82.98\n",
            "Train Epoch: 6 [14500/24574 (58.94%)]\tLoss: 0.40 \t Acc: 83.09\n",
            "Train Epoch: 6 [15000/24574 (60.98%)]\tLoss: 0.40 \t Acc: 83.25\n",
            "Train Epoch: 6 [15500/24574 (63.01%)]\tLoss: 0.40 \t Acc: 83.33\n",
            "Train Epoch: 6 [16000/24574 (65.04%)]\tLoss: 0.39 \t Acc: 83.50\n",
            "Train Epoch: 6 [16500/24574 (67.07%)]\tLoss: 0.39 \t Acc: 83.53\n",
            "Train Epoch: 6 [17000/24574 (69.11%)]\tLoss: 0.39 \t Acc: 83.65\n",
            "Train Epoch: 6 [17500/24574 (71.14%)]\tLoss: 0.39 \t Acc: 83.68\n",
            "Train Epoch: 6 [18000/24574 (73.17%)]\tLoss: 0.39 \t Acc: 83.73\n",
            "Train Epoch: 6 [18500/24574 (75.20%)]\tLoss: 0.39 \t Acc: 83.67\n",
            "Train Epoch: 6 [19000/24574 (77.24%)]\tLoss: 0.39 \t Acc: 83.72\n",
            "Train Epoch: 6 [19500/24574 (79.27%)]\tLoss: 0.39 \t Acc: 83.78\n",
            "Train Epoch: 6 [20000/24574 (81.30%)]\tLoss: 0.39 \t Acc: 83.86\n",
            "Train Epoch: 6 [20500/24574 (83.33%)]\tLoss: 0.38 \t Acc: 83.96\n",
            "Train Epoch: 6 [21000/24574 (85.37%)]\tLoss: 0.38 \t Acc: 84.03\n",
            "Train Epoch: 6 [21500/24574 (87.40%)]\tLoss: 0.38 \t Acc: 84.09\n",
            "Train Epoch: 6 [22000/24574 (89.43%)]\tLoss: 0.38 \t Acc: 84.15\n",
            "Train Epoch: 6 [22500/24574 (91.46%)]\tLoss: 0.38 \t Acc: 84.25\n",
            "Train Epoch: 6 [23000/24574 (93.50%)]\tLoss: 0.38 \t Acc: 84.36\n",
            "Train Epoch: 6 [23500/24574 (95.53%)]\tLoss: 0.38 \t Acc: 84.41\n",
            "Train Epoch: 6 [24000/24574 (97.56%)]\tLoss: 0.37 \t Acc: 84.46\n",
            "Train Epoch: 6 [24500/24574 (99.59%)]\tLoss: 0.37 \t Acc: 84.58\n",
            "\n",
            "Test set: Average loss: 0.005, Accuracy: 7374/8192 (90.01%)\n",
            "\n",
            "Train Epoch: 6, Train loss: 3.6530, Time taken: 45.7699, Train Accuracy: 84.5813, Test loss: 0.0053, Test Accuracy: 90.0146\n",
            "Train Epoch: 7 [500/24574 (2.03%)]\tLoss: 0.27 \t Acc: 91.45\n",
            "Train Epoch: 7 [1000/24574 (4.07%)]\tLoss: 0.26 \t Acc: 91.14\n",
            "Train Epoch: 7 [1500/24574 (6.10%)]\tLoss: 0.26 \t Acc: 90.97\n",
            "Train Epoch: 7 [2000/24574 (8.13%)]\tLoss: 0.25 \t Acc: 90.83\n",
            "Train Epoch: 7 [2500/24574 (10.16%)]\tLoss: 0.25 \t Acc: 90.94\n",
            "Train Epoch: 7 [3000/24574 (12.20%)]\tLoss: 0.25 \t Acc: 90.82\n",
            "Train Epoch: 7 [3500/24574 (14.23%)]\tLoss: 0.25 \t Acc: 91.21\n",
            "Train Epoch: 7 [4000/24574 (16.26%)]\tLoss: 0.24 \t Acc: 91.38\n",
            "Train Epoch: 7 [4500/24574 (18.29%)]\tLoss: 0.24 \t Acc: 91.21\n",
            "Train Epoch: 7 [5000/24574 (20.33%)]\tLoss: 0.24 \t Acc: 91.17\n",
            "Train Epoch: 7 [5500/24574 (22.36%)]\tLoss: 0.23 \t Acc: 91.44\n",
            "Train Epoch: 7 [6000/24574 (24.39%)]\tLoss: 0.23 \t Acc: 91.44\n",
            "Train Epoch: 7 [6500/24574 (26.42%)]\tLoss: 0.24 \t Acc: 91.40\n",
            "Train Epoch: 7 [7000/24574 (28.46%)]\tLoss: 0.23 \t Acc: 91.62\n",
            "Train Epoch: 7 [7500/24574 (30.49%)]\tLoss: 0.23 \t Acc: 91.47\n",
            "Train Epoch: 7 [8000/24574 (32.52%)]\tLoss: 0.23 \t Acc: 91.59\n",
            "Train Epoch: 7 [8500/24574 (34.55%)]\tLoss: 0.23 \t Acc: 91.67\n",
            "Train Epoch: 7 [9000/24574 (36.59%)]\tLoss: 0.23 \t Acc: 91.89\n",
            "Train Epoch: 7 [9500/24574 (38.62%)]\tLoss: 0.22 \t Acc: 91.91\n",
            "Train Epoch: 7 [10000/24574 (40.65%)]\tLoss: 0.22 \t Acc: 92.01\n",
            "Train Epoch: 7 [10500/24574 (42.68%)]\tLoss: 0.22 \t Acc: 92.08\n",
            "Train Epoch: 7 [11000/24574 (44.72%)]\tLoss: 0.22 \t Acc: 91.96\n",
            "Train Epoch: 7 [11500/24574 (46.75%)]\tLoss: 0.22 \t Acc: 92.03\n",
            "Train Epoch: 7 [12000/24574 (48.78%)]\tLoss: 0.22 \t Acc: 92.06\n",
            "Train Epoch: 7 [12500/24574 (50.81%)]\tLoss: 0.21 \t Acc: 92.18\n",
            "Train Epoch: 7 [13000/24574 (52.85%)]\tLoss: 0.21 \t Acc: 92.28\n",
            "Train Epoch: 7 [13500/24574 (54.88%)]\tLoss: 0.21 \t Acc: 92.28\n",
            "Train Epoch: 7 [14000/24574 (56.91%)]\tLoss: 0.21 \t Acc: 92.30\n",
            "Train Epoch: 7 [14500/24574 (58.94%)]\tLoss: 0.21 \t Acc: 92.34\n",
            "Train Epoch: 7 [15000/24574 (60.98%)]\tLoss: 0.21 \t Acc: 92.47\n",
            "Train Epoch: 7 [15500/24574 (63.01%)]\tLoss: 0.21 \t Acc: 92.55\n",
            "Train Epoch: 7 [16000/24574 (65.04%)]\tLoss: 0.20 \t Acc: 92.65\n",
            "Train Epoch: 7 [16500/24574 (67.07%)]\tLoss: 0.20 \t Acc: 92.78\n",
            "Train Epoch: 7 [17000/24574 (69.11%)]\tLoss: 0.20 \t Acc: 92.82\n",
            "Train Epoch: 7 [17500/24574 (71.14%)]\tLoss: 0.20 \t Acc: 92.89\n",
            "Train Epoch: 7 [18000/24574 (73.17%)]\tLoss: 0.19 \t Acc: 92.95\n",
            "Train Epoch: 7 [18500/24574 (75.20%)]\tLoss: 0.19 \t Acc: 92.96\n",
            "Train Epoch: 7 [19000/24574 (77.24%)]\tLoss: 0.19 \t Acc: 93.04\n",
            "Train Epoch: 7 [19500/24574 (79.27%)]\tLoss: 0.19 \t Acc: 93.09\n",
            "Train Epoch: 7 [20000/24574 (81.30%)]\tLoss: 0.19 \t Acc: 93.15\n",
            "Train Epoch: 7 [20500/24574 (83.33%)]\tLoss: 0.19 \t Acc: 93.16\n",
            "Train Epoch: 7 [21000/24574 (85.37%)]\tLoss: 0.19 \t Acc: 93.24\n",
            "Train Epoch: 7 [21500/24574 (87.40%)]\tLoss: 0.19 \t Acc: 93.29\n",
            "Train Epoch: 7 [22000/24574 (89.43%)]\tLoss: 0.18 \t Acc: 93.36\n",
            "Train Epoch: 7 [22500/24574 (91.46%)]\tLoss: 0.18 \t Acc: 93.44\n",
            "Train Epoch: 7 [23000/24574 (93.50%)]\tLoss: 0.18 \t Acc: 93.48\n",
            "Train Epoch: 7 [23500/24574 (95.53%)]\tLoss: 0.18 \t Acc: 93.54\n",
            "Train Epoch: 7 [24000/24574 (97.56%)]\tLoss: 0.18 \t Acc: 93.59\n",
            "Train Epoch: 7 [24500/24574 (99.59%)]\tLoss: 0.18 \t Acc: 93.63\n",
            "\n",
            "Test set: Average loss: 0.002, Accuracy: 7951/8192 (97.06%)\n",
            "\n",
            "Train Epoch: 7, Train loss: 1.7249, Time taken: 46.0079, Train Accuracy: 93.6315, Test loss: 0.0018, Test Accuracy: 97.0581\n",
            "Train Epoch: 8 [500/24574 (2.03%)]\tLoss: 0.13 \t Acc: 95.64\n",
            "Train Epoch: 8 [1000/24574 (4.07%)]\tLoss: 0.10 \t Acc: 97.05\n",
            "Train Epoch: 8 [1500/24574 (6.10%)]\tLoss: 0.09 \t Acc: 97.42\n",
            "Train Epoch: 8 [2000/24574 (8.13%)]\tLoss: 0.10 \t Acc: 96.63\n",
            "Train Epoch: 8 [2500/24574 (10.16%)]\tLoss: 0.11 \t Acc: 96.27\n",
            "Train Epoch: 8 [3000/24574 (12.20%)]\tLoss: 0.12 \t Acc: 96.13\n",
            "Train Epoch: 8 [3500/24574 (14.23%)]\tLoss: 0.12 \t Acc: 96.00\n",
            "Train Epoch: 8 [4000/24574 (16.26%)]\tLoss: 0.12 \t Acc: 96.20\n",
            "Train Epoch: 8 [4500/24574 (18.29%)]\tLoss: 0.12 \t Acc: 96.20\n",
            "Train Epoch: 8 [5000/24574 (20.33%)]\tLoss: 0.11 \t Acc: 96.34\n",
            "Train Epoch: 8 [5500/24574 (22.36%)]\tLoss: 0.11 \t Acc: 96.49\n",
            "Train Epoch: 8 [6000/24574 (24.39%)]\tLoss: 0.10 \t Acc: 96.58\n",
            "Train Epoch: 8 [6500/24574 (26.42%)]\tLoss: 0.11 \t Acc: 96.56\n",
            "Train Epoch: 8 [7000/24574 (28.46%)]\tLoss: 0.10 \t Acc: 96.61\n",
            "Train Epoch: 8 [7500/24574 (30.49%)]\tLoss: 0.10 \t Acc: 96.72\n",
            "Train Epoch: 8 [8000/24574 (32.52%)]\tLoss: 0.10 \t Acc: 96.86\n",
            "Train Epoch: 8 [8500/24574 (34.55%)]\tLoss: 0.10 \t Acc: 96.98\n",
            "Train Epoch: 8 [9000/24574 (36.59%)]\tLoss: 0.09 \t Acc: 97.04\n",
            "Train Epoch: 8 [9500/24574 (38.62%)]\tLoss: 0.09 \t Acc: 97.09\n",
            "Train Epoch: 8 [10000/24574 (40.65%)]\tLoss: 0.09 \t Acc: 97.04\n",
            "Train Epoch: 8 [10500/24574 (42.68%)]\tLoss: 0.09 \t Acc: 97.11\n",
            "Train Epoch: 8 [11000/24574 (44.72%)]\tLoss: 0.09 \t Acc: 97.11\n",
            "Train Epoch: 8 [11500/24574 (46.75%)]\tLoss: 0.09 \t Acc: 97.12\n",
            "Train Epoch: 8 [12000/24574 (48.78%)]\tLoss: 0.09 \t Acc: 97.20\n",
            "Train Epoch: 8 [12500/24574 (50.81%)]\tLoss: 0.09 \t Acc: 97.24\n",
            "Train Epoch: 8 [13000/24574 (52.85%)]\tLoss: 0.08 \t Acc: 97.27\n",
            "Train Epoch: 8 [13500/24574 (54.88%)]\tLoss: 0.08 \t Acc: 97.30\n",
            "Train Epoch: 8 [14000/24574 (56.91%)]\tLoss: 0.08 \t Acc: 97.30\n",
            "Train Epoch: 8 [14500/24574 (58.94%)]\tLoss: 0.08 \t Acc: 97.32\n",
            "Train Epoch: 8 [15000/24574 (60.98%)]\tLoss: 0.09 \t Acc: 97.14\n",
            "Train Epoch: 8 [15500/24574 (63.01%)]\tLoss: 0.09 \t Acc: 97.07\n",
            "Train Epoch: 8 [16000/24574 (65.04%)]\tLoss: 0.09 \t Acc: 97.03\n",
            "Train Epoch: 8 [16500/24574 (67.07%)]\tLoss: 0.09 \t Acc: 97.04\n",
            "Train Epoch: 8 [17000/24574 (69.11%)]\tLoss: 0.09 \t Acc: 96.97\n",
            "Train Epoch: 8 [17500/24574 (71.14%)]\tLoss: 0.09 \t Acc: 97.00\n",
            "Train Epoch: 8 [18000/24574 (73.17%)]\tLoss: 0.09 \t Acc: 97.04\n",
            "Train Epoch: 8 [18500/24574 (75.20%)]\tLoss: 0.09 \t Acc: 97.09\n",
            "Train Epoch: 8 [19000/24574 (77.24%)]\tLoss: 0.09 \t Acc: 97.13\n",
            "Train Epoch: 8 [19500/24574 (79.27%)]\tLoss: 0.08 \t Acc: 97.17\n",
            "Train Epoch: 8 [20000/24574 (81.30%)]\tLoss: 0.08 \t Acc: 97.23\n",
            "Train Epoch: 8 [20500/24574 (83.33%)]\tLoss: 0.08 \t Acc: 97.26\n",
            "Train Epoch: 8 [21000/24574 (85.37%)]\tLoss: 0.08 \t Acc: 97.30\n",
            "Train Epoch: 8 [21500/24574 (87.40%)]\tLoss: 0.08 \t Acc: 97.30\n",
            "Train Epoch: 8 [22000/24574 (89.43%)]\tLoss: 0.08 \t Acc: 97.32\n",
            "Train Epoch: 8 [22500/24574 (91.46%)]\tLoss: 0.08 \t Acc: 97.35\n",
            "Train Epoch: 8 [23000/24574 (93.50%)]\tLoss: 0.08 \t Acc: 97.33\n",
            "Train Epoch: 8 [23500/24574 (95.53%)]\tLoss: 0.08 \t Acc: 97.37\n",
            "Train Epoch: 8 [24000/24574 (97.56%)]\tLoss: 0.08 \t Acc: 97.38\n",
            "Train Epoch: 8 [24500/24574 (99.59%)]\tLoss: 0.08 \t Acc: 97.41\n",
            "\n",
            "Test set: Average loss: 0.001, Accuracy: 8080/8192 (98.63%)\n",
            "\n",
            "Train Epoch: 8, Train loss: 0.7574, Time taken: 53.5015, Train Accuracy: 97.4078, Test loss: 0.0009, Test Accuracy: 98.6328\n",
            "Train Epoch: 9 [500/24574 (2.03%)]\tLoss: 0.04 \t Acc: 99.09\n",
            "Train Epoch: 9 [1000/24574 (4.07%)]\tLoss: 0.05 \t Acc: 98.86\n",
            "Train Epoch: 9 [1500/24574 (6.10%)]\tLoss: 0.05 \t Acc: 98.39\n",
            "Train Epoch: 9 [2000/24574 (8.13%)]\tLoss: 0.06 \t Acc: 98.29\n",
            "Train Epoch: 9 [2500/24574 (10.16%)]\tLoss: 0.05 \t Acc: 98.43\n",
            "Train Epoch: 9 [3000/24574 (12.20%)]\tLoss: 0.05 \t Acc: 98.49\n",
            "Train Epoch: 9 [3500/24574 (14.23%)]\tLoss: 0.05 \t Acc: 98.54\n",
            "Train Epoch: 9 [4000/24574 (16.26%)]\tLoss: 0.05 \t Acc: 98.47\n",
            "Train Epoch: 9 [4500/24574 (18.29%)]\tLoss: 0.05 \t Acc: 98.55\n",
            "Train Epoch: 9 [5000/24574 (20.33%)]\tLoss: 0.04 \t Acc: 98.63\n",
            "Train Epoch: 9 [5500/24574 (22.36%)]\tLoss: 0.04 \t Acc: 98.61\n",
            "Train Epoch: 9 [6000/24574 (24.39%)]\tLoss: 0.05 \t Acc: 98.45\n",
            "Train Epoch: 9 [6500/24574 (26.42%)]\tLoss: 0.05 \t Acc: 98.47\n",
            "Train Epoch: 9 [7000/24574 (28.46%)]\tLoss: 0.05 \t Acc: 98.45\n",
            "Train Epoch: 9 [7500/24574 (30.49%)]\tLoss: 0.05 \t Acc: 98.54\n",
            "Train Epoch: 9 [8000/24574 (32.52%)]\tLoss: 0.05 \t Acc: 98.55\n",
            "Train Epoch: 9 [8500/24574 (34.55%)]\tLoss: 0.05 \t Acc: 98.56\n",
            "Train Epoch: 9 [9000/24574 (36.59%)]\tLoss: 0.05 \t Acc: 98.62\n",
            "Train Epoch: 9 [9500/24574 (38.62%)]\tLoss: 0.04 \t Acc: 98.68\n",
            "Train Epoch: 9 [10000/24574 (40.65%)]\tLoss: 0.04 \t Acc: 98.73\n",
            "Train Epoch: 9 [10500/24574 (42.68%)]\tLoss: 0.04 \t Acc: 98.77\n",
            "Train Epoch: 9 [11000/24574 (44.72%)]\tLoss: 0.04 \t Acc: 98.80\n",
            "Train Epoch: 9 [11500/24574 (46.75%)]\tLoss: 0.04 \t Acc: 98.82\n",
            "Train Epoch: 9 [12000/24574 (48.78%)]\tLoss: 0.04 \t Acc: 98.86\n",
            "Train Epoch: 9 [12500/24574 (50.81%)]\tLoss: 0.04 \t Acc: 98.85\n",
            "Train Epoch: 9 [13000/24574 (52.85%)]\tLoss: 0.04 \t Acc: 98.86\n",
            "Train Epoch: 9 [13500/24574 (54.88%)]\tLoss: 0.04 \t Acc: 98.77\n",
            "Train Epoch: 9 [14000/24574 (56.91%)]\tLoss: 0.04 \t Acc: 98.72\n",
            "Train Epoch: 9 [14500/24574 (58.94%)]\tLoss: 0.04 \t Acc: 98.69\n",
            "Train Epoch: 9 [15000/24574 (60.98%)]\tLoss: 0.04 \t Acc: 98.66\n",
            "Train Epoch: 9 [15500/24574 (63.01%)]\tLoss: 0.04 \t Acc: 98.66\n",
            "Train Epoch: 9 [16000/24574 (65.04%)]\tLoss: 0.04 \t Acc: 98.65\n",
            "Train Epoch: 9 [16500/24574 (67.07%)]\tLoss: 0.04 \t Acc: 98.68\n",
            "Train Epoch: 9 [17000/24574 (69.11%)]\tLoss: 0.04 \t Acc: 98.69\n",
            "Train Epoch: 9 [17500/24574 (71.14%)]\tLoss: 0.04 \t Acc: 98.71\n",
            "Train Epoch: 9 [18000/24574 (73.17%)]\tLoss: 0.04 \t Acc: 98.73\n",
            "Train Epoch: 9 [18500/24574 (75.20%)]\tLoss: 0.04 \t Acc: 98.73\n",
            "Train Epoch: 9 [19000/24574 (77.24%)]\tLoss: 0.04 \t Acc: 98.63\n",
            "Train Epoch: 9 [19500/24574 (79.27%)]\tLoss: 0.05 \t Acc: 98.47\n",
            "Train Epoch: 9 [20000/24574 (81.30%)]\tLoss: 0.05 \t Acc: 98.35\n",
            "Train Epoch: 9 [20500/24574 (83.33%)]\tLoss: 0.05 \t Acc: 98.28\n",
            "Train Epoch: 9 [21000/24574 (85.37%)]\tLoss: 0.05 \t Acc: 98.26\n",
            "Train Epoch: 9 [21500/24574 (87.40%)]\tLoss: 0.05 \t Acc: 98.25\n",
            "Train Epoch: 9 [22000/24574 (89.43%)]\tLoss: 0.05 \t Acc: 98.26\n",
            "Train Epoch: 9 [22500/24574 (91.46%)]\tLoss: 0.05 \t Acc: 98.27\n",
            "Train Epoch: 9 [23000/24574 (93.50%)]\tLoss: 0.05 \t Acc: 98.27\n",
            "Train Epoch: 9 [23500/24574 (95.53%)]\tLoss: 0.05 \t Acc: 98.29\n",
            "Train Epoch: 9 [24000/24574 (97.56%)]\tLoss: 0.05 \t Acc: 98.29\n",
            "Train Epoch: 9 [24500/24574 (99.59%)]\tLoss: 0.05 \t Acc: 98.28\n",
            "\n",
            "Test set: Average loss: 0.001, Accuracy: 8027/8192 (97.99%)\n",
            "\n",
            "Train Epoch: 9, Train loss: 0.5212, Time taken: 46.5155, Train Accuracy: 98.2827, Test loss: 0.0012, Test Accuracy: 97.9858\n",
            "Train Epoch: 10 [500/24574 (2.03%)]\tLoss: 0.07 \t Acc: 97.64\n",
            "Train Epoch: 10 [1000/24574 (4.07%)]\tLoss: 0.08 \t Acc: 97.14\n",
            "Train Epoch: 10 [1500/24574 (6.10%)]\tLoss: 0.07 \t Acc: 97.35\n",
            "Train Epoch: 10 [2000/24574 (8.13%)]\tLoss: 0.07 \t Acc: 97.56\n",
            "Train Epoch: 10 [2500/24574 (10.16%)]\tLoss: 0.06 \t Acc: 97.88\n",
            "Train Epoch: 10 [3000/24574 (12.20%)]\tLoss: 0.05 \t Acc: 98.03\n",
            "Train Epoch: 10 [3500/24574 (14.23%)]\tLoss: 0.05 \t Acc: 98.25\n",
            "Train Epoch: 10 [4000/24574 (16.26%)]\tLoss: 0.05 \t Acc: 98.42\n",
            "Train Epoch: 10 [4500/24574 (18.29%)]\tLoss: 0.04 \t Acc: 98.51\n",
            "Train Epoch: 10 [5000/24574 (20.33%)]\tLoss: 0.04 \t Acc: 98.63\n",
            "Train Epoch: 10 [5500/24574 (22.36%)]\tLoss: 0.04 \t Acc: 98.67\n",
            "Train Epoch: 10 [6000/24574 (24.39%)]\tLoss: 0.04 \t Acc: 98.63\n",
            "Train Epoch: 10 [6500/24574 (26.42%)]\tLoss: 0.04 \t Acc: 98.56\n",
            "Train Epoch: 10 [7000/24574 (28.46%)]\tLoss: 0.04 \t Acc: 98.60\n",
            "Train Epoch: 10 [7500/24574 (30.49%)]\tLoss: 0.04 \t Acc: 98.61\n",
            "Train Epoch: 10 [8000/24574 (32.52%)]\tLoss: 0.04 \t Acc: 98.61\n",
            "Train Epoch: 10 [8500/24574 (34.55%)]\tLoss: 0.04 \t Acc: 98.67\n",
            "Train Epoch: 10 [9000/24574 (36.59%)]\tLoss: 0.04 \t Acc: 98.71\n",
            "Train Epoch: 10 [9500/24574 (38.62%)]\tLoss: 0.04 \t Acc: 98.73\n",
            "Train Epoch: 10 [10000/24574 (40.65%)]\tLoss: 0.03 \t Acc: 98.77\n",
            "Train Epoch: 10 [10500/24574 (42.68%)]\tLoss: 0.03 \t Acc: 98.78\n",
            "Train Epoch: 10 [11000/24574 (44.72%)]\tLoss: 0.03 \t Acc: 98.78\n",
            "Train Epoch: 10 [11500/24574 (46.75%)]\tLoss: 0.03 \t Acc: 98.77\n",
            "Train Epoch: 10 [12000/24574 (48.78%)]\tLoss: 0.03 \t Acc: 98.76\n",
            "Train Epoch: 10 [12500/24574 (50.81%)]\tLoss: 0.03 \t Acc: 98.77\n",
            "Train Epoch: 10 [13000/24574 (52.85%)]\tLoss: 0.03 \t Acc: 98.78\n",
            "Train Epoch: 10 [13500/24574 (54.88%)]\tLoss: 0.03 \t Acc: 98.78\n",
            "Train Epoch: 10 [14000/24574 (56.91%)]\tLoss: 0.03 \t Acc: 98.80\n",
            "Train Epoch: 10 [14500/24574 (58.94%)]\tLoss: 0.03 \t Acc: 98.84\n",
            "Train Epoch: 10 [15000/24574 (60.98%)]\tLoss: 0.03 \t Acc: 98.84\n",
            "Train Epoch: 10 [15500/24574 (63.01%)]\tLoss: 0.03 \t Acc: 98.84\n",
            "Train Epoch: 10 [16000/24574 (65.04%)]\tLoss: 0.03 \t Acc: 98.85\n",
            "Train Epoch: 10 [16500/24574 (67.07%)]\tLoss: 0.03 \t Acc: 98.87\n",
            "Train Epoch: 10 [17000/24574 (69.11%)]\tLoss: 0.03 \t Acc: 98.89\n",
            "Train Epoch: 10 [17500/24574 (71.14%)]\tLoss: 0.03 \t Acc: 98.90\n",
            "Train Epoch: 10 [18000/24574 (73.17%)]\tLoss: 0.03 \t Acc: 98.93\n",
            "Train Epoch: 10 [18500/24574 (75.20%)]\tLoss: 0.03 \t Acc: 98.95\n",
            "Train Epoch: 10 [19000/24574 (77.24%)]\tLoss: 0.03 \t Acc: 98.97\n",
            "Train Epoch: 10 [19500/24574 (79.27%)]\tLoss: 0.03 \t Acc: 98.97\n",
            "Train Epoch: 10 [20000/24574 (81.30%)]\tLoss: 0.03 \t Acc: 98.99\n",
            "Train Epoch: 10 [20500/24574 (83.33%)]\tLoss: 0.03 \t Acc: 98.97\n",
            "Train Epoch: 10 [21000/24574 (85.37%)]\tLoss: 0.03 \t Acc: 98.94\n",
            "Train Epoch: 10 [21500/24574 (87.40%)]\tLoss: 0.03 \t Acc: 98.91\n",
            "Train Epoch: 10 [22000/24574 (89.43%)]\tLoss: 0.03 \t Acc: 98.92\n",
            "Train Epoch: 10 [22500/24574 (91.46%)]\tLoss: 0.03 \t Acc: 98.92\n",
            "Train Epoch: 10 [23000/24574 (93.50%)]\tLoss: 0.03 \t Acc: 98.93\n",
            "Train Epoch: 10 [23500/24574 (95.53%)]\tLoss: 0.03 \t Acc: 98.94\n",
            "Train Epoch: 10 [24000/24574 (97.56%)]\tLoss: 0.03 \t Acc: 98.91\n",
            "Train Epoch: 10 [24500/24574 (99.59%)]\tLoss: 0.03 \t Acc: 98.88\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-362b8b52408d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mexec_time\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         message = ('Train Epoch: {}, Train loss: {:.4f}, Time taken: {:.4f}, Train Accuracy: {:.4f}, Test loss: {:.4f}, Test Accuracy: {:.4f}' .format(\n\u001b[1;32m     11\u001b[0m                 epoch, train_loss, t, train_acc, test_loss, test_acc))\n",
            "\u001b[0;32m<ipython-input-46-34cf4dc11b9e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-2cbd7774fee7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6b5b85a8df9e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_t, tuple_in)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Equation 2. forget gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Equation 3. updating the cell memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-6b5b85a8df9e>\u001b[0m in \u001b[0;36mforget\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_forget_w1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_forget_r1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_forget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minput_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "604836ed"
      },
      "outputs": [],
      "source": [
        "plt.plot(accuracy)\n",
        "plt.title('Accuracy Vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')"
      ],
      "id": "604836ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adccc9ba"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.title('Loss Vs Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')"
      ],
      "id": "adccc9ba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d18f793"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for segments, labels in test_loader:\n",
        "        segments = segments.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(segments)\n",
        "\n",
        "        # max return(value, index)\n",
        "\n",
        "        predicted_classes = outputs.round()\n",
        "        #print(predicted)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted_classes== labels).sum().item()\n",
        "\n",
        "    acc = 100.0*n_correct/n_samples\n",
        "    print(f'Accuracy of the network for ioctal and preictal class: {acc:.3f}%')"
      ],
      "id": "3d18f793"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "762da125"
      },
      "outputs": [],
      "source": [],
      "id": "762da125"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}