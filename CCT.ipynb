{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fplfIeOo7TzP",
        "outputId": "fb949148-c3cf-431e-ad58-5aa624cfa053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RV1hGj-4TW7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from functools import partial\n",
        "import time\n",
        "from einops import rearrange, repeat\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import OrderedDict\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIgR6tZL8TMh",
        "outputId": "6ee36d97-69fb-4d39-c2a9-0e5922645a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "UzQt5-iS9naP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(dataset, windowlen):\n",
        "    \"\"\"Transform a time series into a prediction dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        lookback: Size of window for prediction\n",
        "    \"\"\"\n",
        "    # feature_scalling\n",
        "    sc = StandardScaler()\n",
        "    signal = sc.fit_transform(dataset)\n",
        "    X = []\n",
        "    for i in range(int(len(signal)/windowlen)):\n",
        "        feature = signal[i*windowlen:(i+1)*windowlen]\n",
        "        X.append(feature)\n",
        "    return torch.tensor(np.array(X)).float()\n",
        "\n",
        "def data_generator( batch_size, windowlen):\n",
        "    print('Loading CHB-MIT Interical and preictal dataset...')\n",
        "    preictal_data = pd.read_csv('/content/drive/MyDrive/data/ictal_data.csv')\n",
        "    ictal_data = pd.read_csv('/content/drive/MyDrive/data/preictal_data.csv')\n",
        "\n",
        "    class1 = create_dataset(preictal_data, windowlen=windowlen)\n",
        "    y_1= torch.zeros(class1.shape[0],1)\n",
        "\n",
        "    class2 = create_dataset(ictal_data, windowlen=windowlen)\n",
        "    y_2 = torch.ones(class2.shape[0],1)\n",
        "\n",
        "    datasets = torch.cat((class1, class2),0)\n",
        "    labels = torch.cat((y_1, y_2), 0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(datasets, labels, test_size=0.25, shuffle=True, random_state=42)\n",
        "    print(f\" Shape of the Training data is {X_train.shape,}, and Testing data is {X_test.shape}\" )\n",
        "\n",
        "    train_loader = data.DataLoader(data.TensorDataset(X_train, y_train),  batch_size=batch_size, shuffle=True)\n",
        "    test_loader = data.DataLoader(data.TensorDataset(X_test, y_test),  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "fGcn1iTGKdhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=50\n",
        "window_length=512"
      ],
      "metadata": {
        "id": "2abdyiHvxU6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = data_generator(batch_size, window_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIPe4gvgKsOq",
        "outputId": "75f5a0d7-40e6-480d-b58a-1b6a7185cb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CHB-MIT Interical and preictal dataset...\n",
            " Shape of the Training data is (torch.Size([3070, 512, 23]),), and Testing data is torch.Size([1024, 512, 23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.001\n",
        "d_dim=23\n",
        "heads=2\n",
        "epochs=30\n",
        "embed_dim=40\n",
        "num_layers=1\n",
        "dropout=0.1\n",
        "log_interval=10\n",
        "\n",
        "\n",
        "n_classes = 1  ## For Binary class\n",
        "sequence_length = window_length"
      ],
      "metadata": {
        "id": "oWSgxI0Exbdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.register_buffer(\"beta\", torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, dropout):\n",
        "        super(Residual, self).__init__()\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        \"Residual connection with the same size.\"\n",
        "        return x + self.dropout(self.norm(x))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout):\n",
        "        super(MLP,self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    def __init__(self, drop_prob):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.drop_prob = torch.Tensor([drop_prob])\n",
        "        self.register_buffer('Drop out', self.drop_prob)\n",
        "\n",
        "    def forward(self, x, training=None):\n",
        "        if self.training:\n",
        "            device = x.device\n",
        "            keep_prob = (1 - self.drop_prob).to(device)\n",
        "            shape = (x.size(0),) + (1,) * (x.dim() - 1)\n",
        "            random_tensor = keep_prob + torch.rand(shape).to(device)\n",
        "            random_tensor = random_tensor.floor()\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, d_dim, embed_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.d_dim = d_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed = nn.Linear(self.d_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "metadata": {
        "id": "XSPYpBjT4tuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequencePooling(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(SequencePooling, self).__init__()\n",
        "        self.attention = nn.Linear(in_features, out_features=1)\n",
        "        self.apply(self.init_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_weights = F.softmax(self.attention(x), dim=1)\n",
        "        attention_weights = torch.transpose(attention_weights, 1, 2)\n",
        "        weighted_representation = torch.matmul(attention_weights, x)\n",
        "        return torch.squeeze(weighted_representation, dim=-2)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weight(m):\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.trunc_normal_(m.weight, std=0.1)\n",
        "        nn.init.constant_(m.bias,0)\n",
        "\n",
        "\n",
        "class Conv_Tokenizer(nn.Module):\n",
        "   def __init__(self):\n",
        "    super(Conv_Tokenizer, self).__init__()\n",
        "    self.conv = nn.Sequential(OrderedDict([\n",
        "        ('conv1', nn.Conv2d(1, 3, 3, 1, 1)),\n",
        "        ('relu1', nn.ReLU()),\n",
        "        ('max1', nn.MaxPool2d(3, 1, 1)),\n",
        "        ('conv2', nn.Conv2d(3, 1, 3, 1, 1)),\n",
        "        ('relu2', nn.ReLU()),\n",
        "        ('max2', nn.MaxPool2d(3, 1, 1))]))\n",
        "    self.apply(self.initialize_weight)\n",
        "\n",
        "   def forward(self,x):\n",
        "    return self.conv(x)\n",
        "\n",
        "   @staticmethod\n",
        "   def initialize_weight(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "      nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, sequence_length, dim):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(sequence_length, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(x.size(-1)).expand(x.size(-2), -1).to(device)\n",
        "        return x + self.embedding(positions)\n",
        "\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.heads = heads\n",
        "        self.dim = dim\n",
        "\n",
        "        assert self.dim% self.heads==0, 'dim is not a factor of head_dim'\n",
        "        self.head_dim = self.dim/self.heads\n",
        "\n",
        "        inner_dim = int(self.head_dim *  self.heads)\n",
        "        self.scale = self.head_dim  ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(self.dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, self.dim, bias= False),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.apply(self.initialize_weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        # Attention Score\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize_weight(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class LCT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        positional_emb=True,\n",
        "        stochastic_depth_rate=0.1,\n",
        "        dropout = 0.1,\n",
        "        transformer_layers=6,\n",
        "        num_classes=5,\n",
        "    ):\n",
        "        super(LCT, self).__init__()\n",
        "\n",
        "        self.embedding = Embedding(input_shape[-1],embed_dim)  # Embedding of input tensor\n",
        "        input_shape = (input_shape[0],input_shape[1], embed_dim)\n",
        "\n",
        "\n",
        "        self.conv_tokenizer = Conv_Tokenizer()\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "        if positional_emb:\n",
        "            self.position_embedding = PositionEmbedding(sequence_length=input_shape[-2], dim = input_shape[-1])\n",
        "\n",
        "        self.transformer_layers = transformer_layers\n",
        "        self.stochastic_depth_rate = stochastic_depth_rate\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Calculate Stochastic Depth probabilities.\n",
        "        dpr = [x.item() for x in torch.linspace(0, self.stochastic_depth_rate, self.transformer_layers)]\n",
        "\n",
        "        self.transformer_blocks = nn.Sequential()\n",
        "        for i in range(transformer_layers):\n",
        "            # Layer normalization 1.\n",
        "            self.transformer_blocks.append(nn.LayerNorm(input_shape[-1]))\n",
        "\n",
        "            # Create a multi-head attention layer.\n",
        "            self.transformer_blocks.append(\n",
        "                MHA(dim=input_shape[-1], heads=num_heads, dropout=0.1)\n",
        "            )\n",
        "\n",
        "            # Skip connection 1.\n",
        "            self.transformer_blocks.append(StochasticDepth(dpr[i]))\n",
        "\n",
        "            # Layer normalization 2.\n",
        "            self.transformer_blocks.append(Residual(input_shape[-1],self.dropout))\n",
        "\n",
        "            # MLP.\n",
        "            self.transformer_blocks.append(MLP(dim=input_shape[-1], hidden_dim=20, dropout=self.dropout))\n",
        "\n",
        "            # Skip connection 2.\n",
        "            self.transformer_blocks.append(Residual(input_shape[-1],self.dropout))\n",
        "\n",
        "        #self.ct_trans = nn.Sequential(*self.transformer_blocks)\n",
        "\n",
        "        self.sequence_pooling = SequencePooling(input_shape[-1])  # Placeholder for SequencePooling\n",
        "\n",
        "        self.classifier = nn.Linear(input_shape[-1], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed data.\n",
        "        data_embed = self.embedding(x)\n",
        "\n",
        "        # Convolution Tokenizer.\n",
        "        data_conv = self.conv_tokenizer(data_embed.unsqueeze(1)).squeeze()\n",
        "\n",
        "        # Apply positional embedding.\n",
        "        if self.positional_emb:\n",
        "            data_conv = self.position_embedding(data_conv)\n",
        "\n",
        "        data = self.transformer_blocks(data_conv)\n",
        "\n",
        "        # Apply sequence pooling.\n",
        "        weighted_representation = self.sequence_pooling(data)\n",
        "\n",
        "        # Classify outputs.\n",
        "        out = self.classifier(weighted_representation)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "zz8TseyX4yAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1, leb = next(iter(train_loader))\n",
        "input_shape = data1.shape\n",
        "print('Input shape', data1.shape)\n",
        "\n",
        "class LCTmodel(nn.Module):\n",
        "    def __init__(self, input_shape, embed_dim, num_heads, positional_emb, stochastic_depth_rate,\n",
        "            dropout, transformer_layers, num_classes):\n",
        "        super(LCTmodel, self).__init__()\n",
        "\n",
        "        self.Trans = LCT(input_shape,\n",
        "                embed_dim,\n",
        "                num_heads,\n",
        "                positional_emb,\n",
        "                stochastic_depth_rate,\n",
        "                dropout,\n",
        "                transformer_layers,\n",
        "                num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.Trans(x)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwE--1w9FTsQ",
        "outputId": "2c9ca14a-eb1d-4340-f906-21711fe36d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape torch.Size([50, 512, 23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model =  LCTmodel(input_shape,\n",
        "        embed_dim=40,\n",
        "        num_heads=4,\n",
        "        positional_emb=True,\n",
        "        stochastic_depth_rate=0.1,\n",
        "        dropout = 0.1,\n",
        "        transformer_layers=1,\n",
        "        num_classes=1).to(device)\n",
        "\n",
        "\n",
        "model_name = \"Model_{}_dim_{}_heads_{}_lr_{}_dropout_{}\".format(\n",
        "            'CCT',embed_dim, heads, lr, dropout)\n",
        "\n",
        "message_filename =  'r_' + model_name + '.txt'\n",
        "model_filename =  'm_' + model_name + '.pt'\n",
        "with open(message_filename, 'w') as out:\n",
        "    out.write('start\\n')"
      ],
      "metadata": {
        "id": "6E2jlYfJ7AM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_s(message, save_filename):\n",
        "    print (message)\n",
        "    with open(save_filename, 'a') as out:\n",
        "        out.write(message + '\\n')\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "\n",
        "\n",
        "def train(ep):\n",
        "    targets = list()\n",
        "    preds = list()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "        pred = output.round()\n",
        "        correct += (pred== target).sum().item()\n",
        "        targets += list(target.detach().cpu().numpy())\n",
        "        preds += list(pred.detach().cpu().numpy())\n",
        "        acc = 100. * correct / ((batch_idx+1) * batch_size)\n",
        "\n",
        "        if batch_idx > 0 and batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.2f} \\t Acc: {:.2f}\".format(\n",
        "                ep, batch_idx * batch_size, len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), train_loss.item()/(batch_idx),acc))\n",
        "\n",
        "    return 100. * correct / len(train_loader.dataset), train_loss.item()/batch_size,\n",
        "\n",
        "\n",
        "## Leeanable parameters counts ###\n",
        "def test():\n",
        "    model.eval()\n",
        "\n",
        "    targets = list()\n",
        "    preds = list()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.round()\n",
        "            correct += (pred== target).sum().item()\n",
        "            targets += list(target.detach().cpu().numpy())\n",
        "            preds += list(pred.detach().cpu().numpy())\n",
        "\n",
        "        Acc = 100. * correct / len(test_loader.dataset)\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss: {:.3f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset), Acc))\n",
        "        #output_s(message, message_filename)\n",
        "        return targets, preds, Acc, test_loss\n"
      ],
      "metadata": {
        "id": "nEue-MVBtynn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    exec_time = 0\n",
        "    for epoch in range(1, epochs+1):\n",
        "        start = time.time()\n",
        "        train_acc, train_loss = train(epoch)\n",
        "        end = time.time()\n",
        "        t = end-start\n",
        "        exec_time+= t\n",
        "        preds, targets, test_acc, test_loss = test()\n",
        "        message = ('Train Epoch: {}, Train loss: {:.4f}, Time taken: {:.4f}, Train Accuracy: {:.4f}, Test loss: {:.4f}, Test Accuracy: {:.4f}' .format(\n",
        "                epoch, train_loss, t, train_acc, test_loss, test_acc))\n",
        "        output_s(message, message_filename)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            lr /= 10\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        if epoch%(epochs)==0:\n",
        "            print('Total Execution time for training:',exec_time)\n",
        "            preds = np.array(preds)\n",
        "            targets = np.array(targets)\n",
        "            conf_mat= confusion_matrix(targets, preds)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix= conf_mat)\n",
        "            disp.plot()\n",
        "            print(classification_report(targets, preds, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "D8gc0_P4P-mW",
        "outputId": "64154e5c-46d2-44a5-9d1c-020fc6743f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [500/3070 (16.13%)]\tLoss: 0.01 \t Acc: 99.64\n",
            "Train Epoch: 1 [1000/3070 (32.26%)]\tLoss: 0.01 \t Acc: 99.71\n",
            "Train Epoch: 1 [1500/3070 (48.39%)]\tLoss: 0.01 \t Acc: 99.81\n",
            "Train Epoch: 1 [2000/3070 (64.52%)]\tLoss: 0.01 \t Acc: 99.80\n",
            "Train Epoch: 1 [2500/3070 (80.65%)]\tLoss: 0.01 \t Acc: 99.69\n",
            "Train Epoch: 1 [3000/3070 (96.77%)]\tLoss: 0.02 \t Acc: 99.57\n",
            "\n",
            "Test set: Average loss: 0.002, Accuracy: 992/1024 (96.88%)\n",
            "\n",
            "Train Epoch: 1, Train loss: 0.0231, Time taken: 2.3300, Train Accuracy: 99.5765, Test loss: 0.0021, Test Accuracy: 96.8750\n",
            "Train Epoch: 2 [500/3070 (16.13%)]\tLoss: 0.03 \t Acc: 99.64\n",
            "Train Epoch: 2 [1000/3070 (32.26%)]\tLoss: 0.02 \t Acc: 99.71\n",
            "Train Epoch: 2 [1500/3070 (48.39%)]\tLoss: 0.02 \t Acc: 99.55\n",
            "Train Epoch: 2 [2000/3070 (64.52%)]\tLoss: 0.02 \t Acc: 99.46\n",
            "Train Epoch: 2 [2500/3070 (80.65%)]\tLoss: 0.02 \t Acc: 99.57\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-362b8b52408d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-ace424fd2694>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ep)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}