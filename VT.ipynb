{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRA8BXUZsD85"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import sys, os\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.utils.data as data\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmZ5_bBUsJAR",
        "outputId": "3a1107f3-decd-4a50-d04a-f7e11174c303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "N4YFdy_NsJDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(dataset, windowlen):\n",
        "    \"\"\"Transform a time series into a prediction dataset\n",
        "\n",
        "    Args:\n",
        "        dataset: A numpy array of time series, first dimension is the time steps\n",
        "        lookback: Size of window for prediction\n",
        "    \"\"\"\n",
        "    # feature_scalling\n",
        "    sc = StandardScaler()\n",
        "    signal = sc.fit_transform(dataset)\n",
        "    X = []\n",
        "    for i in range(int(len(signal)/windowlen)):\n",
        "        feature = signal[i*windowlen:(i+1)*windowlen]\n",
        "        X.append(feature)\n",
        "    return torch.tensor(np.array(X)).float()\n",
        "\n",
        "def data_generator( batch_size, windowlen):\n",
        "    print('Loading CHB-MIT Interical and preictal dataset...')\n",
        "    preictal_data = pd.read_csv('/content/drive/MyDrive/data/ictal_data.csv')\n",
        "    ictal_data = pd.read_csv('/content/drive/MyDrive/data/preictal_data.csv')\n",
        "\n",
        "    class1 = create_dataset(preictal_data, windowlen=windowlen)\n",
        "    y_1= torch.zeros(class1.shape[0],1)\n",
        "\n",
        "    class2 = create_dataset(ictal_data, windowlen=windowlen)\n",
        "    y_2 = torch.ones(class2.shape[0],1)\n",
        "\n",
        "    datasets = torch.cat((class1, class2),0)\n",
        "    labels = torch.cat((y_1, y_2), 0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(datasets, labels, test_size=0.25, shuffle=True, random_state=42)\n",
        "    print(f\" Shape of the Training data is {X_train.shape,}, and Testing data is {X_test.shape}\" )\n",
        "\n",
        "    train_loader = data.DataLoader(data.TensorDataset(X_train, y_train),  batch_size=batch_size, shuffle=True)\n",
        "    test_loader = data.DataLoader(data.TensorDataset(X_test, y_test),  batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "vlK_qkGbsO3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =50\n",
        "window_length = 512\n",
        "train_loader, test_loader = data_generator(batch_size, window_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE_5ZGnlsO6X",
        "outputId": "9b38a50c-6564-4219-83af-304b970759b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CHB-MIT Interical and preictal dataset...\n",
            " Shape of the Training data is (torch.Size([3070, 512, 23]),), and Testing data is torch.Size([1024, 512, 23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.001\n",
        "d_dim=23\n",
        "heads=2\n",
        "epochs=10\n",
        "embed_dim=40\n",
        "num_layers=2\n",
        "dropout=0.1\n",
        "log_interval=10\n",
        "n_classes = 1  ## For Binary class\n",
        "sequence_length = window_length\n"
      ],
      "metadata": {
        "id": "w3W9Bb1VsO8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, d_dim, embed_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.d_dim = d_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed = nn.Linear(self.d_dim, self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "\n",
        "def positional_embedding(sequence_length, embed_dim):\n",
        "    '''\n",
        "    Args:\n",
        "        sequence_length: length of the input Sequence\n",
        "        embed_dim: Embedding dimension\n",
        "    '''\n",
        "    pe = torch.zeros(sequence_length, embed_dim)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(embed_dim):\n",
        "            pe[i][j] = np.sin(i / (10000 ** (j / embed_dim))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / embed_dim)))\n",
        "\n",
        "    return pe.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim: dimension of the embedding vector output\n",
        "            n_heads: number of self attention heads\n",
        "        \"\"\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert self.embed_dim% self.n_heads == 0, 'n_heads is not a factor of Embed_dim '\n",
        "        self.single_head_dim = int(self.embed_dim/self.n_heads)\n",
        "\n",
        "        # Defining key, query, and value matrixes\n",
        "        self.key_mat = nn.Linear(self.single_head_dim, self.single_head_dim, bias = False)\n",
        "        self.query_mat = nn.Linear(self.single_head_dim, self.single_head_dim, bias = False)\n",
        "        self.value_mat = nn.Linear(self.single_head_dim, self.single_head_dim, bias = False)\n",
        "        self.W_o_mat = nn.Linear(self.n_heads*self.single_head_dim, self.embed_dim, bias = False)\n",
        "\n",
        "\n",
        "    def forward (self, key, query, value, need_weights=False): # batch_size x sequence_length x embedding_dim\n",
        "        '''\n",
        "        Args:\n",
        "            key: key vector\n",
        "            query: query vector\n",
        "            value: value vector\n",
        "\n",
        "        returns:\n",
        "            output vectors from multihead attention\n",
        "\n",
        "        '''\n",
        "        batch_size = key.shape[0]\n",
        "        sequence_length = key.shape[1]\n",
        "\n",
        "        # Changing dimensions according to heads\n",
        "        key = key.view(batch_size, sequence_length, self.n_heads, self.single_head_dim)\n",
        "        query = query.view(batch_size, sequence_length, self.n_heads, self.single_head_dim)\n",
        "        value = value.view(batch_size, sequence_length, self.n_heads, self.single_head_dim)\n",
        "\n",
        "        # Computing key, query, and value vectors\n",
        "        K = self.key_mat(key)\n",
        "        Q = self.query_mat(query)\n",
        "        V = self.value_mat(value)\n",
        "\n",
        "        # Transposing the matrixes to find the multihead self attention\n",
        "\n",
        "        K = K.transpose(1,2)\n",
        "        Q = Q.transpose(1,2)\n",
        "        V = V.transpose(1,2)\n",
        "\n",
        "        # Attention computation\n",
        "        # adjusting key for matrix multiplication\n",
        "        K_adj = K.transpose(-1,-2) # (batch_size, n_heads, single_head_dim, sequence_length)\n",
        "\n",
        "        product = Q@K_adj\n",
        "\n",
        "        # scaling the product\n",
        "        product = product/self.single_head_dim**0.5\n",
        "\n",
        "        # normalizing the product\n",
        "        scores = F.softmax(product, dim = -1)\n",
        "\n",
        "        # Computing the self attention\n",
        "        attn = scores@V\n",
        "\n",
        "        # Concatenated output\n",
        "        concat = attn.transpose(1,2).contiguous().view(batch_size, sequence_length, self.n_heads*self.single_head_dim)\n",
        "\n",
        "        output = self.W_o_mat(concat)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # First layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        # Multi-head attention\n",
        "\n",
        "        self.msa = MultiHeadAttention(embed_dim, n_heads)\n",
        "\n",
        "        # Second layer Normalization\n",
        "        self.norm2 = nn.LayerNorm((embed_dim))\n",
        "\n",
        "        # Encoder Multilayer perceptron\n",
        "        self.feed_forward = nn.Sequential(nn.Linear(embed_dim, embed_dim)\n",
        "                                          ,nn.ReLU())\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x1):\n",
        "\n",
        "        x = self.norm1(x1)\n",
        "\n",
        "        attention = self.msa(x,x,x, need_weights=False) #  Multi-head attention\n",
        "        attention = attention+x # Residual Connection\n",
        "        out = self.dropout1(attention)\n",
        "\n",
        "        out = self.norm2(out)\n",
        "        out = self.feed_forward(out)+out # Feed-forward with residual connection\n",
        "\n",
        "        return self.dropout2(out)\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim,d_dim, n_heads, num_layers, pos_enc):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.d_dim = d_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.pos_enc = pos_enc\n",
        "\n",
        "        #self.tokenizer = Tokenizer()\n",
        "        self.embed = Embedding(self.d_dim, self.embed_dim)\n",
        "\n",
        "        # positional Embedding\n",
        "\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerBlock(self.embed_dim, self.n_heads)\n",
        "                                    for i in range(num_layers)])\n",
        "\n",
        "        self.fc1 = nn.Linear(self.sequence_length*self.embed_dim, self.embed_dim)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "    def forward(self, x):\n",
        "        #tokens = self.tokenizer(x)\n",
        "        out = self.embed(x)\n",
        "        if self.pos_enc:\n",
        "            out += positional_embedding(self.sequence_length, self.embed_dim)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        # Attention based pooling\n",
        "        #attn = self.attn(out)\n",
        "        #attention_out = attn.transpose(2,1)@out\n",
        "\n",
        "        # Avergae pooling\n",
        "        representation = torch.flatten(out, start_dim=1)\n",
        "        representation = self.fc1(representation)\n",
        "        representation = self.dropout3(representation)\n",
        "        return representation"
      ],
      "metadata": {
        "id": "3F1McPoRsO_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vanilla_Transformer(nn.Module):\n",
        "    def __init__(self, sequence_length, embed_dim, d_dim, n_heads, num_class, num_layers, pos_enc= False):\n",
        "        super(Vanilla_Transformer, self).__init__()\n",
        "\n",
        "        self.Trans = TransformerEncoder(sequence_length,\n",
        "            embed_dim,d_dim, n_heads, num_layers, pos_enc)\n",
        "        self.linear = nn.Linear(embed_dim, num_class)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.Trans(x)\n",
        "        out = self.sigmoid(self.linear(out))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "twNOjoKh1vEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =  Vanilla_Transformer(sequence_length=512, embed_dim= 30,d_dim=23, n_heads=2, num_class=1,num_layers=2, pos_enc= False).to(device)"
      ],
      "metadata": {
        "id": "usoLAobk1xuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Model_{}_dim_{}_heads_{}_lr_{}_dropout_{}\".format(\n",
        "            'VT',embed_dim, heads, lr, dropout)\n",
        "\n",
        "message_filename =  'r_' + model_name + '.txt'\n",
        "model_filename =  'm_' + model_name + '.pt'\n",
        "with open(message_filename, 'w') as out:\n",
        "    out.write('start\\n')\n",
        "\n",
        "\n",
        "def output_s(message, save_filename):\n",
        "    print (message)\n",
        "    with open(save_filename, 'a') as out:\n",
        "        out.write(message + '\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "gH98-eH82HmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n",
        "\n",
        "def train(ep):\n",
        "    targets = list()\n",
        "    preds = list()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "        pred = output.round()\n",
        "        correct += (pred== target).sum().item()\n",
        "        targets += list(target.detach().cpu().numpy())\n",
        "        preds += list(pred.detach().cpu().numpy())\n",
        "        acc = 100. * correct / ((batch_idx+1) * batch_size)\n",
        "\n",
        "        if batch_idx > 0 and batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.2f} \\t Acc: {:.2f}\".format(\n",
        "                ep, batch_idx * batch_size, len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), train_loss.item()/(batch_idx),acc))\n",
        "\n",
        "    return 100. * correct / len(train_loader.dataset), train_loss.item()/batch_size,\n",
        "\n",
        "\n",
        "## Leeanable parameters counts ###\n",
        "def test():\n",
        "    model.eval()\n",
        "\n",
        "    targets = list()\n",
        "    preds = list()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target)\n",
        "            pred = output.round()\n",
        "            correct += (pred== target).sum().item()\n",
        "            targets += list(target.detach().cpu().numpy())\n",
        "            preds += list(pred.detach().cpu().numpy())\n",
        "\n",
        "        Acc = 100. * correct / len(test_loader.dataset)\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss: {:.3f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset), Acc))\n",
        "        #output_s(message, message_filename)\n",
        "        return targets, preds, Acc, test_loss\n",
        "\n",
        "#model_total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# In[112]:"
      ],
      "metadata": {
        "id": "_nXDIuSQOrB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    exec_time = 0\n",
        "    for epoch in range(1, epochs+1):\n",
        "        start = time.time()\n",
        "        train_acc, train_loss = train(epoch)\n",
        "        end = time.time()\n",
        "        t = end-start\n",
        "        exec_time+= t\n",
        "        preds, targets, test_acc, test_loss = test()\n",
        "        message = ('Train Epoch: {}, Train loss: {:.4f}, Time taken: {:.4f}, Train Accuracy: {:.4f}, Test loss: {:.4f}, Test Accuracy: {:.4f}' .format(\n",
        "                epoch, train_loss, t, train_acc, test_loss, test_acc))\n",
        "        output_s(message, message_filename)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            lr /= 10\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        if epoch%(epochs)==0:\n",
        "            print('Total Execution time for training:',exec_time)\n",
        "            preds = np.array(preds)\n",
        "            targets = np.array(targets)\n",
        "            conf_mat= confusion_matrix(targets, preds)\n",
        "            disp = ConfusionMatrixDisplay(confusion_matrix= conf_mat)\n",
        "            disp.plot()\n",
        "            print(classification_report(targets, preds, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "65NHwBhp3wnV",
        "outputId": "ae626bb0-c913-4cac-9415-c340c55c45fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [500/3070 (16.13%)]\tLoss: 1.65 \t Acc: 50.91\n",
            "Train Epoch: 1 [1000/3070 (32.26%)]\tLoss: 1.45 \t Acc: 50.95\n",
            "Train Epoch: 1 [1500/3070 (48.39%)]\tLoss: 1.32 \t Acc: 52.58\n",
            "Train Epoch: 1 [2000/3070 (64.52%)]\tLoss: 1.25 \t Acc: 53.02\n",
            "Train Epoch: 1 [2500/3070 (80.65%)]\tLoss: 1.18 \t Acc: 53.80\n",
            "Train Epoch: 1 [3000/3070 (96.77%)]\tLoss: 1.11 \t Acc: 54.07\n",
            "\n",
            "Test set: Average loss: 0.016, Accuracy: 634/1024 (61.91%)\n",
            "\n",
            "Train Epoch: 1, Train loss: 1.3466, Time taken: 2.7179, Train Accuracy: 54.2020, Test loss: 0.0155, Test Accuracy: 61.9141\n",
            "Train Epoch: 2 [500/3070 (16.13%)]\tLoss: 0.58 \t Acc: 74.91\n",
            "Train Epoch: 2 [1000/3070 (32.26%)]\tLoss: 0.57 \t Acc: 73.33\n",
            "Train Epoch: 2 [1500/3070 (48.39%)]\tLoss: 0.56 \t Acc: 73.48\n",
            "Train Epoch: 2 [2000/3070 (64.52%)]\tLoss: 0.56 \t Acc: 73.32\n",
            "Train Epoch: 2 [2500/3070 (80.65%)]\tLoss: 0.54 \t Acc: 73.80\n",
            "Train Epoch: 2 [3000/3070 (96.77%)]\tLoss: 0.54 \t Acc: 74.07\n",
            "\n",
            "Test set: Average loss: 0.013, Accuracy: 741/1024 (72.36%)\n",
            "\n",
            "Train Epoch: 2, Train loss: 0.6557, Time taken: 1.8714, Train Accuracy: 74.1368, Test loss: 0.0126, Test Accuracy: 72.3633\n",
            "Train Epoch: 3 [500/3070 (16.13%)]\tLoss: 0.47 \t Acc: 80.55\n",
            "Train Epoch: 3 [1000/3070 (32.26%)]\tLoss: 0.46 \t Acc: 80.29\n",
            "Train Epoch: 3 [1500/3070 (48.39%)]\tLoss: 0.46 \t Acc: 80.52\n",
            "Train Epoch: 3 [2000/3070 (64.52%)]\tLoss: 0.44 \t Acc: 81.17\n",
            "Train Epoch: 3 [2500/3070 (80.65%)]\tLoss: 0.43 \t Acc: 81.45\n",
            "Train Epoch: 3 [3000/3070 (96.77%)]\tLoss: 0.43 \t Acc: 81.84\n",
            "\n",
            "Test set: Average loss: 0.013, Accuracy: 762/1024 (74.41%)\n",
            "\n",
            "Train Epoch: 3, Train loss: 0.5149, Time taken: 1.7934, Train Accuracy: 81.9218, Test loss: 0.0131, Test Accuracy: 74.4141\n",
            "Train Epoch: 4 [500/3070 (16.13%)]\tLoss: 0.32 \t Acc: 88.55\n",
            "Train Epoch: 4 [1000/3070 (32.26%)]\tLoss: 0.30 \t Acc: 88.10\n",
            "Train Epoch: 4 [1500/3070 (48.39%)]\tLoss: 0.30 \t Acc: 88.32\n",
            "Train Epoch: 4 [2000/3070 (64.52%)]\tLoss: 0.30 \t Acc: 88.00\n",
            "Train Epoch: 4 [2500/3070 (80.65%)]\tLoss: 0.30 \t Acc: 87.80\n",
            "Train Epoch: 4 [3000/3070 (96.77%)]\tLoss: 0.30 \t Acc: 88.07\n",
            "\n",
            "Test set: Average loss: 0.010, Accuracy: 850/1024 (83.01%)\n",
            "\n",
            "Train Epoch: 4, Train loss: 0.3607, Time taken: 1.7980, Train Accuracy: 88.0130, Test loss: 0.0095, Test Accuracy: 83.0078\n",
            "Train Epoch: 5 [500/3070 (16.13%)]\tLoss: 0.22 \t Acc: 92.18\n",
            "Train Epoch: 5 [1000/3070 (32.26%)]\tLoss: 0.22 \t Acc: 91.71\n",
            "Train Epoch: 5 [1500/3070 (48.39%)]\tLoss: 0.21 \t Acc: 92.06\n",
            "Train Epoch: 5 [2000/3070 (64.52%)]\tLoss: 0.22 \t Acc: 91.61\n",
            "Train Epoch: 5 [2500/3070 (80.65%)]\tLoss: 0.22 \t Acc: 91.92\n",
            "Train Epoch: 5 [3000/3070 (96.77%)]\tLoss: 0.21 \t Acc: 91.97\n",
            "\n",
            "Test set: Average loss: 0.010, Accuracy: 864/1024 (84.38%)\n",
            "\n",
            "Train Epoch: 5, Train loss: 0.2628, Time taken: 1.8088, Train Accuracy: 91.9218, Test loss: 0.0097, Test Accuracy: 84.3750\n",
            "Train Epoch: 6 [500/3070 (16.13%)]\tLoss: 0.21 \t Acc: 93.27\n",
            "Train Epoch: 6 [1000/3070 (32.26%)]\tLoss: 0.22 \t Acc: 92.10\n",
            "Train Epoch: 6 [1500/3070 (48.39%)]\tLoss: 0.20 \t Acc: 92.26\n",
            "Train Epoch: 6 [2000/3070 (64.52%)]\tLoss: 0.19 \t Acc: 92.44\n",
            "Train Epoch: 6 [2500/3070 (80.65%)]\tLoss: 0.19 \t Acc: 92.67\n",
            "Train Epoch: 6 [3000/3070 (96.77%)]\tLoss: 0.19 \t Acc: 92.75\n",
            "\n",
            "Test set: Average loss: 0.007, Accuracy: 905/1024 (88.38%)\n",
            "\n",
            "Train Epoch: 6, Train loss: 0.2253, Time taken: 1.8089, Train Accuracy: 92.7687, Test loss: 0.0068, Test Accuracy: 88.3789\n",
            "Train Epoch: 7 [500/3070 (16.13%)]\tLoss: 0.11 \t Acc: 96.73\n",
            "Train Epoch: 7 [1000/3070 (32.26%)]\tLoss: 0.11 \t Acc: 96.00\n",
            "Train Epoch: 7 [1500/3070 (48.39%)]\tLoss: 0.12 \t Acc: 95.35\n",
            "Train Epoch: 7 [2000/3070 (64.52%)]\tLoss: 0.13 \t Acc: 95.27\n",
            "Train Epoch: 7 [2500/3070 (80.65%)]\tLoss: 0.14 \t Acc: 95.18\n",
            "Train Epoch: 7 [3000/3070 (96.77%)]\tLoss: 0.14 \t Acc: 94.95\n",
            "\n",
            "Test set: Average loss: 0.007, Accuracy: 909/1024 (88.77%)\n",
            "\n",
            "Train Epoch: 7, Train loss: 0.1731, Time taken: 1.8169, Train Accuracy: 94.9511, Test loss: 0.0071, Test Accuracy: 88.7695\n",
            "Train Epoch: 8 [500/3070 (16.13%)]\tLoss: 0.09 \t Acc: 97.27\n",
            "Train Epoch: 8 [1000/3070 (32.26%)]\tLoss: 0.08 \t Acc: 97.33\n",
            "Train Epoch: 8 [1500/3070 (48.39%)]\tLoss: 0.09 \t Acc: 97.10\n",
            "Train Epoch: 8 [2000/3070 (64.52%)]\tLoss: 0.09 \t Acc: 97.12\n",
            "Train Epoch: 8 [2500/3070 (80.65%)]\tLoss: 0.09 \t Acc: 96.94\n",
            "Train Epoch: 8 [3000/3070 (96.77%)]\tLoss: 0.08 \t Acc: 97.05\n",
            "\n",
            "Test set: Average loss: 0.008, Accuracy: 903/1024 (88.18%)\n",
            "\n",
            "Train Epoch: 8, Train loss: 0.1038, Time taken: 1.8573, Train Accuracy: 96.9707, Test loss: 0.0081, Test Accuracy: 88.1836\n",
            "Train Epoch: 9 [500/3070 (16.13%)]\tLoss: 0.08 \t Acc: 97.27\n",
            "Train Epoch: 9 [1000/3070 (32.26%)]\tLoss: 0.08 \t Acc: 97.24\n",
            "Train Epoch: 9 [1500/3070 (48.39%)]\tLoss: 0.07 \t Acc: 97.55\n",
            "Train Epoch: 9 [2000/3070 (64.52%)]\tLoss: 0.07 \t Acc: 97.37\n",
            "Train Epoch: 9 [2500/3070 (80.65%)]\tLoss: 0.06 \t Acc: 97.61\n",
            "Train Epoch: 9 [3000/3070 (96.77%)]\tLoss: 0.07 \t Acc: 97.48\n",
            "\n",
            "Test set: Average loss: 0.006, Accuracy: 935/1024 (91.31%)\n",
            "\n",
            "Train Epoch: 9, Train loss: 0.0790, Time taken: 1.7993, Train Accuracy: 97.4919, Test loss: 0.0060, Test Accuracy: 91.3086\n",
            "Train Epoch: 10 [500/3070 (16.13%)]\tLoss: 0.04 \t Acc: 98.73\n",
            "Train Epoch: 10 [1000/3070 (32.26%)]\tLoss: 0.05 \t Acc: 98.10\n",
            "Train Epoch: 10 [1500/3070 (48.39%)]\tLoss: 0.06 \t Acc: 97.81\n",
            "Train Epoch: 10 [2000/3070 (64.52%)]\tLoss: 0.05 \t Acc: 97.90\n",
            "Train Epoch: 10 [2500/3070 (80.65%)]\tLoss: 0.05 \t Acc: 97.84\n",
            "Train Epoch: 10 [3000/3070 (96.77%)]\tLoss: 0.05 \t Acc: 98.03\n",
            "\n",
            "Test set: Average loss: 0.013, Accuracy: 904/1024 (88.28%)\n",
            "\n",
            "Train Epoch: 10, Train loss: 0.0644, Time taken: 1.8113, Train Accuracy: 98.0456, Test loss: 0.0131, Test Accuracy: 88.2812\n",
            "Total Execution time for training: 19.08329176902771\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7996    0.9777    0.8798       449\n",
            "         1.0     0.9789    0.8087    0.8857       575\n",
            "\n",
            "    accuracy                         0.8828      1024\n",
            "   macro avg     0.8893    0.8932    0.8827      1024\n",
            "weighted avg     0.9003    0.8828    0.8831      1024\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}